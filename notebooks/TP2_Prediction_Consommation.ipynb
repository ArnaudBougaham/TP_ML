{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwIAnRRJ3izu"
      },
      "source": [
        "# TP Machine Learning - Industrie Sidérurgique\n",
        "## Partie 2 : Prédiction de la consommation énergétique\n",
        "\n",
        "Dans cette partie, nous allons développer des modèles de machine learning pour prédire la consommation énergétique\n",
        "d'une industrie sidérurgique. Cette capacité de prédiction est cruciale pour l'optimisation des coûts et la\n",
        "planification de la production.\n",
        "\n",
        "### Objectifs :\n",
        "- Implémenter différents modèles de régression\n",
        "- Comparer leurs performances\n",
        "- Interpréter les résultats dans un contexte industriel\n",
        "\n",
        "### Structure des données :\n",
        "\n",
        "1. **Variable cible (à prédire)** :\n",
        "   - `Usage_kWh` : Consommation d'énergie en kilowattheures\n",
        "\n",
        "2. **Variables descriptives numériques** :\n",
        "   - `Lagging_Current_Reactive.Power_kVarh` : Puissance réactive en retard\n",
        "   - `Leading_Current_Reactive_Power_kVarh` : Puissance réactive en avance\n",
        "   - `CO2(tCO2)` : Émissions de CO2\n",
        "   - `Lagging_Current_Power_Factor` : Facteur de puissance en retard\n",
        "   - `Leading_Current_Power_Factor` : Facteur de puissance en avance\n",
        "   - `NSM` : Number of Seconds from Midnight (temps)\n",
        "\n",
        "3. **Variables catégorielles** :\n",
        "   - `Day_of_week` : Jour de la semaine (Monday à Sunday)\n",
        "   - `WeekStatus` : Type de jour (Weekday/Weekend)\n",
        "\n",
        "### Prétraitement appliqué :\n",
        "1. Standardisation des variables numériques (moyenne=0, écart-type=1)\n",
        "2. Encodage one-hot des variables catégorielles\n",
        "3. Division train/test (80%/20%)\n",
        "\n",
        "### Modèles abordés :\n",
        "- Régression linéaire (baseline)\n",
        "- K plus proches voisins (KNN)\n",
        "- Arbres de décision\n",
        "- Random Forest\n",
        "- Réseaux de neurones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC74lzPz3nlb"
      },
      "outputs": [],
      "source": [
        "# Import des packages nécessaires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                           explained_variance_score, max_error)\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from scipy import stats\n",
        "import os\n",
        "\n",
        "# Configuration de l'affichage\n",
        "sns.set_theme()  # Utilisation du style seaborn directement\n",
        "plt.rcParams['figure.figsize'] = [10, 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWz0Bfck3xSw"
      },
      "outputs": [],
      "source": [
        "# Téléchargement et préparation des données\n",
        "if not os.path.exists('Steel_industry_data.csv'):\n",
        "    print(\"Téléchargement des données...\")\n",
        "    # Télécharger le fichier zip\n",
        "    !wget -O steel_industry_data.zip https://archive.ics.uci.edu/static/public/851/steel+industry+energy+consumption.zip\n",
        "    # Décompresser le fichier\n",
        "    !unzip -o steel_industry_data.zip\n",
        "    print(\"Données téléchargées et décompressées.\")\n",
        "else:\n",
        "    print(\"Fichier de données déjà présent.\")\n",
        "\n",
        "# Chargement des données\n",
        "try:\n",
        "    df = pd.read_csv('Steel_industry_data.csv')\n",
        "    print(f\"Données chargées avec succès : {df.shape[0]} observations, {df.shape[1]} variables\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement des données : {e}\")\n",
        "    raise\n",
        "\n",
        "# Séparation des variables\n",
        "target = 'Usage_kWh'\n",
        "numeric_features = [\n",
        "    'Lagging_Current_Reactive.Power_kVarh',\n",
        "    'Leading_Current_Reactive_Power_kVarh',\n",
        "    'CO2(tCO2)',\n",
        "    'Lagging_Current_Power_Factor',\n",
        "    'Leading_Current_Power_Factor',\n",
        "    'NSM'\n",
        "]\n",
        "categorical_features = ['Day_of_week', 'WeekStatus', 'period']\n",
        "\n",
        "# Affichage des dimensions\n",
        "print(\"Dimensions du dataset :\")\n",
        "print(f\"Nombre d'observations : {df.shape[0]:,}\")\n",
        "print(f\"Nombre de variables : {df.shape[1]:,}\")\n",
        "\n",
        "# Résumé statistique des variables numériques\n",
        "print(\"\\nRésumé statistique des variables numériques :\")\n",
        "display(df[numeric_features + [target]].describe())\n",
        "\n",
        "# Distribution de la variable cible\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x=target, bins=50)\n",
        "# /!\\ Complétez les '...' pour afficher comme titre de la figure (Matplotlib): Distribution de la consommation énergétique /!\\\n",
        "plt...\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsktu0Ukxxz9"
      },
      "source": [
        "### 1. Analyse temporelle et création des caractéristiques\n",
        "\n",
        "Nous allons structurer nos données temporelles en plusieurs niveaux :\n",
        "1. Jours de la semaine (lundi à dimanche)\n",
        "2. Type de jour (semaine/weekend)\n",
        "3. Périodes de la journée (6 blocs de 4h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jo6Hh2Hx45f"
      },
      "outputs": [],
      "source": [
        "# Création des périodes de la journée industrielle\n",
        "def create_industrial_periods(df):\n",
        "    # Conversion NSM en heures\n",
        "    df['hour'] = df['NSM'] / 3600\n",
        "\n",
        "    # Création des périodes avec la journée commençant à 6h\n",
        "    conditions = [\n",
        "        (df['hour'] >= 6) & (df['hour'] < 10),   # Matin1\n",
        "        (df['hour'] >= 10) & (df['hour'] < 14),  # Matin2\n",
        "        (df['hour'] >= 14) & (df['hour'] < 18),  # Aprem1\n",
        "        (df['hour'] >= 18) & (df['hour'] < 22),  # Aprem2\n",
        "        (df['hour'] >= 22) | (df['hour'] < 2),   # Nuit1\n",
        "        (df['hour'] >= 2) & (df['hour'] < 6)     # Nuit2\n",
        "    ]\n",
        "\n",
        "    periods = ['Matin1', 'Matin2', 'Aprem1', 'Aprem2', 'Nuit1', 'Nuit2']\n",
        "    df['period'] = np.select(conditions, periods, default='Nuit2')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Application des périodes\n",
        "# /!\\ Complétez les '...' pour transformer df grâce à la fonction create_industrial_periods() /!\\\n",
        "df = ...\n",
        "\n",
        "# Visualisation des patterns de consommation\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 1. Consommation moyenne par période\n",
        "plt.subplot(1, 3, 1)\n",
        "period_order = ['Matin1', 'Matin2', 'Aprem1', 'Aprem2', 'Nuit1', 'Nuit2']\n",
        "sns.boxplot(data=df, x='period', y='Usage_kWh', order=period_order)\n",
        "plt.title('Distribution de la consommation par période')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 2. Heatmap période x jour\n",
        "pivot_period_day = pd.pivot_table(df,\n",
        "                                values='Usage_kWh',\n",
        "                                index='period',\n",
        "                                columns='Day_of_week',\n",
        "                                aggfunc='mean')\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(pivot_period_day, cmap='YlOrRd', annot=True, fmt='.0f')\n",
        "plt.title('Consommation moyenne\\npar période et jour')\n",
        "\n",
        "# 3. Comparaison semaine/weekend\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.boxplot(data=df, x='period', y='Usage_kWh', hue='WeekStatus', order=period_order)\n",
        "plt.title('Consommation par période\\net type de jour')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistiques détaillées\n",
        "print(\"\\nConsommation moyenne (kWh) par période et type de jour :\")\n",
        "pivot_stats = pd.pivot_table(df,\n",
        "                           values='Usage_kWh',\n",
        "                           index='period',\n",
        "                           columns=['WeekStatus', 'Day_of_week'],\n",
        "                           aggfunc=['mean', 'std'])\n",
        "display(pivot_stats.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsBi5wc9x8ye"
      },
      "source": [
        "❓ **Questions sur les patterns temporels :**\n",
        "\n",
        "1. **Cycles de production**\n",
        "   - Quelle période montre la plus forte consommation ? Pourquoi ?\n",
        "   - Comment évolue la consommation entre Matin1 et Matin2 ?\n",
        "\n",
        "2. **Variations jour/nuit**\n",
        "   - Quelle est la différence de consommation entre périodes diurnes et nocturnes ?\n",
        "   - La variabilité est-elle plus importante le jour ou la nuit ?\n",
        "\n",
        "3. **Impact weekend**\n",
        "   - Comment le pattern de consommation change-t-il le weekend ?\n",
        "   - Quelles périodes montrent la plus grande différence semaine/weekend ?\n",
        "   - Quelles recommandations pour l'optimisation énergétique ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HK6q4HHxwku"
      },
      "outputs": [],
      "source": [
        "# Préparation des données\n",
        "\n",
        "# 1. Standardisation des variables numériques\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df[numeric_features]),\n",
        "    columns=numeric_features\n",
        ")\n",
        "\n",
        "# 2. Encodage des variables catégorielles\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "categorical_encoded = encoder.fit_transform(df[categorical_features])\n",
        "\n",
        "# Noms des colonnes encodées\n",
        "day_names = [f'Day_{day}' for day in encoder.categories_[0]]\n",
        "week_status_names = [f'Status_{status}' for status in encoder.categories_[1]]\n",
        "period_names = [f'Period_{period}' for period in encoder.categories_[2]]\n",
        "encoded_columns = day_names + week_status_names + period_names\n",
        "\n",
        "# Vérification des dimensions\n",
        "print(\"\\nDimensions de l'encodage :\")\n",
        "print(f\"Nombre de colonnes encodées : {len(encoded_columns)}\")\n",
        "print(f\"Shape des données encodées : {categorical_encoded.shape}\")\n",
        "print(\"Catégories encodées :\")\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    print(f\"{feature}: {list(encoder.categories_[i])}\")\n",
        "\n",
        "# Création du DataFrame avec les variables encodées\n",
        "df_encoded = pd.DataFrame(categorical_encoded, columns=encoded_columns)\n",
        "\n",
        "# 3. Combinaison des features\n",
        "X = pd.concat([df_scaled, df_encoded], axis=1)\n",
        "# /!\\ Complétez les '...' pour affecter la variable y à la colonne target du dataframe /!\\\n",
        "y = ...\n",
        "\n",
        "print(\"Structure des données préparées :\")\n",
        "print(f\"Variables numériques standardisées : {len(numeric_features)}\")\n",
        "print(f\"Variables catégorielles encodées : {len(encoded_columns)}\")\n",
        "print(f\"Dimensions finales de X : {X.shape}\")\n",
        "\n",
        "# 4. Division train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nDimensions des ensembles d'entraînement et de test :\")\n",
        "print(f\"X_train : {X_train.shape}\")\n",
        "print(f\"X_test : {X_test.shape}\")\n",
        "print(f\"y_train : {y_train.shape}\")\n",
        "print(f\"y_test : {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzGq21B45DO"
      },
      "source": [
        "### 2. Régression linéaire\n",
        "\n",
        "Pour comprendre comment la régression linéaire trouve ses coefficients, implémentons notre propre version :\n",
        "\n",
        "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
        "\n",
        "La loss (erreur quadratique moyenne) est :\n",
        "L = (1/n) Σ(y_pred - y_true)²\n",
        "\n",
        "Les coefficients sont mis à jour selon :\n",
        "β_new = β_old - α * ∂L/∂β\n",
        "où α est le taux d'apprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUrczzCX46-W"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        self.weights_history = []\n",
        "        self.bias_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialisation des paramètres\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Historique pour visualisation\n",
        "        self.loss_history = []\n",
        "        self.weights_history = []\n",
        "        self.bias_history = []\n",
        "\n",
        "        # Descente de gradient\n",
        "        for i in range(self.n_iterations):\n",
        "            # Prédiction courante\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Calcul des gradients\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Mise à jour des paramètres\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            # Calcul et sauvegarde de la loss et des paramètres\n",
        "            loss = np.mean((y_pred - y) ** 2)\n",
        "            self.loss_history.append(loss)\n",
        "            self.weights_history.append(self.weights.copy())\n",
        "            self.bias_history.append(self.bias)\n",
        "\n",
        "            # Affichage progression\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(f'Iteration {i+1}/{self.n_iterations}, Loss: {loss:.4f}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Visualisation de l'erreur en fonction de chaque poids\n",
        "def plot_error_vs_weights(X, y, weights, bias, feature_names, n_points=100):\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    n_features = len(weights)\n",
        "    n_rows = (n_features + 3) // 4  # Arrondi supérieur pour le nombre de lignes\n",
        "\n",
        "    for i, (feature_name, weight) in enumerate(zip(feature_names, weights)):\n",
        "        # Créer une plage de valeurs autour du poids optimal\n",
        "        weight_range = np.linspace(weight - 2, weight + 2, n_points)\n",
        "        errors = []\n",
        "\n",
        "        # Calculer l'erreur pour chaque valeur du poids\n",
        "        for w in weight_range:\n",
        "            weights_temp = weights.copy()\n",
        "            weights_temp[i] = w\n",
        "            y_pred = np.dot(X, weights_temp) + bias\n",
        "            mse = np.mean((y - y_pred) ** 2)\n",
        "            errors.append(mse)\n",
        "\n",
        "        # Tracer la courbe d'erreur\n",
        "        plt.subplot(n_rows, 4, i+1)\n",
        "        plt.plot(weight_range, errors, 'b-', alpha=0.7)\n",
        "        plt.axvline(x=weight, color='r', linestyle='--', label=f'w={weight:.2f}')\n",
        "        plt.title(f'MSE vs {feature_name}')\n",
        "        plt.xlabel('Valeur du poids')\n",
        "        plt.ylabel('MSE')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Marquer le poids trouvé\n",
        "        plt.plot(weight, np.min(errors), 'ro', label='Poids trouvé')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Entraînement des deux modèles\n",
        "lr_gd = LinearRegressionGD(learning_rate=0.01, n_iterations=15000)\n",
        "# /!\\ Complétez les '...' pour appliquer la régression linéaire lr_gd aux données /!\\\n",
        "...(X_train.values, y_train.values)\n",
        "\n",
        "lr_sk = LinearRegression()\n",
        "# /!\\ Complétez les '...' pour appliquer la régression linéaire lr_sk aux données /!\\\n",
        "...(X_train, y_train)\n",
        "\n",
        "# Comparaison des performances\n",
        "print(\"\\nComparaison des deux implémentations :\")\n",
        "comparison = pd.DataFrame(columns=['GD', 'Sklearn'])\n",
        "\n",
        "# Prédictions\n",
        "y_pred_gd = lr_gd.predict(X_test.values)\n",
        "y_pred_sk = lr_sk.predict(X_test)\n",
        "\n",
        "# Métriques\n",
        "comparison.loc['R² score'] = [\n",
        "    r2_score(y_test, y_pred_gd),\n",
        "    r2_score(y_test, y_pred_sk)\n",
        "]\n",
        "comparison.loc['MSE'] = [\n",
        "    mean_squared_error(y_test, y_pred_gd),\n",
        "    mean_squared_error(y_test, y_pred_sk)\n",
        "]\n",
        "comparison.loc['RMSE'] = [\n",
        "    np.sqrt(mean_squared_error(y_test, y_pred_gd)),\n",
        "    np.sqrt(mean_squared_error(y_test, y_pred_sk))\n",
        "]\n",
        "\n",
        "print(\"\\nMétriques de performance :\")\n",
        "display(comparison.round(4))\n",
        "\n",
        "# Comparaison des coefficients\n",
        "coef_comparison = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'GD': lr_gd.weights,\n",
        "    'Sklearn': lr_sk.coef_,\n",
        "    'Différence': np.abs(lr_gd.weights - lr_sk.coef_)\n",
        "})\n",
        "\n",
        "print(\"\\nComparaison des coefficients :\")\n",
        "display(coef_comparison.round(4))\n",
        "\n",
        "# Affichage de l'équation complète (avec les 5 coefficients les plus importants)\n",
        "print(\"\\nÉquation de régression :\")\n",
        "print(f\"Usage_kWh = {lr_gd.bias:.2f}\", end=\" \")\n",
        "top_coefs = coef_comparison.assign(abs_coef=lambda x: np.abs(x['GD'])).nlargest(21, 'abs_coef')\n",
        "for _, row in top_coefs.iterrows():\n",
        "    print(f\"+ ({row['GD']:.2f} × {row['Feature']})\", end=\" \")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualisation des courbes d'erreur\n",
        "plot_error_vs_weights(X_train.values, y_train.values,\n",
        "                     lr_gd.weights, lr_gd.bias,\n",
        "                     feature_names=X_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32orerj2vryv"
      },
      "source": [
        "### Interprétation des métriques :\n",
        "\n",
        "1. **R² (Coefficient de détermination)**\n",
        "   - Varie entre 0 et 1 (ou négatif si le modèle est très mauvais)\n",
        "   - Plus il est proche de 1, meilleur est le modèle\n",
        "   - Représente la proportion de variance expliquée par le modèle\n",
        "   - Un R² de 0.8 signifie que le modèle explique 80% de la variabilité des données\n",
        "\n",
        "2. **MSE (Mean Squared Error)**\n",
        "   - Moyenne des erreurs au carré\n",
        "   - Pénalise fortement les grandes erreurs\n",
        "   - Difficile à interpréter car unité au carré\n",
        "\n",
        "3. **RMSE (Root Mean Squared Error)**\n",
        "   - Racine carrée du MSE\n",
        "   - Même unité que la variable cible (kWh)\n",
        "   - Plus facile à interpréter : erreur moyenne en kWh\n",
        "   - Exemple : RMSE = 10 signifie une erreur moyenne de ±10 kWh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-Zk-4T49kJ"
      },
      "source": [
        "❓ **Questions approfondies sur la régression linéaire :**\n",
        "\n",
        "1. **Convergence**\n",
        "   - Comment évolue la loss au fil des itérations ?\n",
        "   - Pourquoi la diminution est-elle rapide au début puis plus lente ?\n",
        "   - Comment savoir si on a atteint le minimum global ?\n",
        "\n",
        "2. **Comparaison avec sklearn**\n",
        "   - Les coefficients sont-ils similaires ?\n",
        "   - Pourquoi y a-t-il des différences ?\n",
        "\n",
        "3. **Compréhension mathématique**\n",
        "   - Pourquoi utilise-t-on la MSE (Mean Squared Error) comme fonction de perte ?\n",
        "\n",
        "4. **Analyse des coefficients**\n",
        "   - Pourquoi CO2(tCO2) a-t-il le plus grand coefficient (26.47) ?\n",
        "   - Les coefficients négatifs signifient-ils une influence négative ?\n",
        "\n",
        "5. **Courbes d'erreur**\n",
        "   - Pourquoi les courbes ont-elles une forme parabolique ?\n",
        "   - Que signifie la largeur de la parabole pour chaque feature ?\n",
        "   - Pourquoi certaines features ont-elles un impact plus important ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNF3Ufc44_9N"
      },
      "source": [
        "### 3. K plus proches voisins (KNN)\n",
        "L'algorithme des K plus proches voisins est une méthode non-paramétrique qui prédit la consommation\n",
        "en se basant sur les k observations les plus similaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqn-ohkS5CpK"
      },
      "outputs": [],
      "source": [
        "class SimpleKNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Mémorise simplement les données d'entraînement\"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        print(f\"Mémorisation de {len(X)} observations d'entraînement\")\n",
        "\n",
        "    def predict_single(self, x, verbose=False):\n",
        "        \"\"\"Prédit pour une seule observation avec option d'affichage des détails\"\"\"\n",
        "        # Calcul des distances avec tous les points d'entraînement\n",
        "        distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
        "\n",
        "        # Trouve les k plus proches voisins\n",
        "        nearest_indices = np.argsort(distances)[:self.k]\n",
        "        nearest_distances = distances[nearest_indices]\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\nDétails de la prédiction:\")\n",
        "            print(f\"Observation à prédire: {x}\")\n",
        "            print(\"\\nPlus proches voisins trouvés:\")\n",
        "            for i, (idx, dist) in enumerate(zip(nearest_indices, nearest_distances)):\n",
        "                print(f\"Voisin {i+1}:\")\n",
        "                print(f\"- Distance: {dist:.2f}\")\n",
        "                print(f\"- Valeur: {self.y_train[idx]:.2f}\")\n",
        "\n",
        "        # Calcul de la prédiction (moyenne simple)\n",
        "        prediction = np.mean(self.y_train[nearest_indices])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nPrédiction finale: {prediction:.2f}\")\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Prédit pour plusieurs observations\"\"\"\n",
        "        return np.array([self.predict_single(x) for x in X])\n",
        "\n",
        "# Test avec différentes valeurs de k\n",
        "k_values = [1, 2, 3, 4, 5]\n",
        "knn_scores = []\n",
        "\n",
        "# Création d'un exemple simple pour visualisation\n",
        "example_idx = 42  # Un indice arbitraire pour l'exemple\n",
        "\n",
        "for k in k_values:\n",
        "    knn = SimpleKNN(k=k)\n",
        "    knn.fit(X_train.values, y_train.values)\n",
        "\n",
        "    # Prédiction détaillée pour l'exemple\n",
        "    print(f\"\\nTest avec k={k}:\")\n",
        "    example_pred = knn.predict_single(X_test.values[example_idx], verbose=True)\n",
        "\n",
        "    # Calcul des métriques\n",
        "    y_pred = knn.predict(X_test.values)\n",
        "    # /!\\ Complétez les '...' pour calculer le R² entre la valeur cible réel et la prédiction /!\\\n",
        "    r2 = ...\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    knn_scores.append(r2)\n",
        "\n",
        "    print(f\"\\nMétriques globales:\")\n",
        "    print(f\"R² score: {r2:.3f}\")\n",
        "    print(f\"MSE: {mse:.3f}\")\n",
        "    print(f\"RMSE: {rmse:.3f}\")\n",
        "\n",
        "# Visualisation de l'impact de k\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_values, knn_scores, 'bo-')\n",
        "plt.xlabel('Nombre de voisins (k)')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance selon le nombre de voisins')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1MFZ0r5FOY"
      },
      "source": [
        "❓ **Questions sur l'algorithme KNN :**\n",
        "\n",
        "1. **Compréhension de l'algorithme**\n",
        "   - Comment le KNN fait-il ses prédictions pour une nouvelle observation ?\n",
        "   - Pourquoi est-il important que les variables soient standardisées ?\n",
        "\n",
        "2. **Choix de k**\n",
        "   - Que se passe-t-il si k est trop petit (k=1) ?\n",
        "   - Que se passe-t-il si k est trop grand (k proche de n) ?\n",
        "   - Pourquoi observe-t-on un k optimal dans la courbe de performance ?\n",
        "\n",
        "3. **Interprétabilité**\n",
        "   - Comment expliquer une prédiction KNN à un utilisateur ?\n",
        "   - Peut-on identifier les variables les plus importantes ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htfMqB5h5J9b"
      },
      "source": [
        "### 4. Arbres de décision\n",
        "Les arbres de décision permettent de créer des règles de prédiction facilement interprétables.\n",
        "Ils peuvent capturer des relations non linéaires et sont particulièrement utiles dans un contexte industriel.\n",
        "\n",
        "Points clés :\n",
        "- Modèle transparent et interprétable\n",
        "- Capable de capturer des relations non linéaires\n",
        "- Risque de surapprentissage à contrôler\n",
        "\n",
        "L'arbre de décision divise récursivement les données en sous-groupes homogènes\n",
        "en choisissant les meilleures variables et seuils de séparation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKbXBp1F5Kqb"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Création et entraînement de l'arbre avec profondeur 4\n",
        "dt = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Visualisation de l'arbre avec une meilleure lisibilité\n",
        "plt.figure(figsize=(30, 15))  # Grande taille pour la lisibilité\n",
        "plot_tree(dt,\n",
        "          feature_names=X_train.columns,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=8,  # Taille de la police\n",
        "          precision=2)  # Nombre de décimales pour les valeurs\n",
        "plt.title('Arbre de décision (profondeur=8)', fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "# Exemple de prédiction détaillée\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "prediction = dt.predict([example])[0]\n",
        "\n",
        "print(\"\\nDétails de l'instance à prédire:\")\n",
        "print(f\"\\nValeur réelle de consommation: {real_value:.2f} kWh\")\n",
        "print(f\"Valeur prédite: {prediction:.2f} kWh\")\n",
        "\n",
        "print(\"\\nCaractéristiques non nulles de l'instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # On n'affiche que les valeurs non nulles\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "print(\"\\nChemin de décision détaillé:\")\n",
        "\n",
        "print(\"\\nNiveau 1:\")\n",
        "print(\"   Test: CO2(tCO2) ≤ 0.22\")\n",
        "print(\"   Valeur mesurée: -0.71\")\n",
        "print(\"   Nombre d'observations: 28032\")\n",
        "print(\"   Moyenne du groupe: 27.29 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\nNiveau 2:\")\n",
        "print(\"   Test: CO2(tCO2) ≤ -0.40\")\n",
        "print(\"   Valeur mesurée: -0.71\")\n",
        "print(\"   Nombre d'observations: 18049\")\n",
        "print(\"   Moyenne du groupe: 5.41 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\nNiveau 3:\")\n",
        "print(\"   Test: Lagging_Current_Reactive.Power_kVarh ≤ 0.60\")\n",
        "print(\"   Valeur mesurée: -0.60\")\n",
        "print(\"   Nombre d'observations: 16817\")\n",
        "print(\"   Moyenne du groupe: 3.91 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\nNiveau 4:\")\n",
        "print(\"   Test: Lagging_Current_Reactive.Power_kVarh ≤ -0.1\")\n",
        "print(\"   Valeur mesurée: -0.60\")\n",
        "print(\"   Nombre d'observations: 16793\")\n",
        "print(\"   Moyenne du groupe: 3.76 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\n→ Feuille finale:\")\n",
        "print(\"   - Nombre d'observations: 16788\")\n",
        "print(\"   - Valeur prédite: 3.75 kWh\")\n",
        "\n",
        "# Métriques de performance\n",
        "y_pred = dt.predict(X_test)\n",
        "print(\"\\nMétriques de performance:\")\n",
        "print(f\"R² score: {r2_score(y_test, y_pred):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f}\")\n",
        "\n",
        "\n",
        "# Création et entraînement de l'arbre avec profondeur 10\n",
        "# /!\\ Complétez les '...' pour entrainer un arbre de décision d'une profondeur de 10 /!\\\n",
        "dt = ...\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Métriques de performance\n",
        "y_pred = dt.predict(X_test)\n",
        "print(\"\\nMétriques de performance pour profondeur 10:\")\n",
        "print(f\"R² score: {r2_score(y_test, y_pred):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6VT5BrJ5N3B"
      },
      "source": [
        "❓ **Questions :**\n",
        "1. Quelles sont les variables les plus importantes selon l'arbre de décision ?\n",
        "\n",
        "2. Comment la profondeur de l'arbre influence-t-elle les performances ?\n",
        "\n",
        "3. **Structure de l'arbre**\n",
        "   - Pourquoi CO2(tCO2) est-il souvent choisi comme première division ?\n",
        "   - Comment le nombre d'observations diminue-t-il à chaque niveau ?\n",
        "   - Quelle est la signification des valeurs moyennes dans les nœuds ?\n",
        "\n",
        "4. **Processus de prédiction**\n",
        "   - Comment l'arbre arrive-t-il à sa prédiction finale ?\n",
        "   - Pourquoi les prédictions sont-elles plus précises avec la profondeur ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxRKnsE-5SOX"
      },
      "source": [
        "### 5. Random Forest\n",
        "Le Random Forest est un ensemble d'arbres de décision qui permet d'améliorer la généralisation\n",
        "et la stabilité des prédictions par rapport à un arbre unique.\n",
        "\n",
        "Points clés :\n",
        "- Meilleure généralisation que l'arbre simple\n",
        "- Estimation robuste de l'importance des variables\n",
        "- Réduction du surapprentissage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lClrkkG15S6Z"
      },
      "outputs": [],
      "source": [
        "# Création d'un Random Forest simple avec 500 arbres pour la visualisation\n",
        "rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Exemple de prédiction détaillée\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "\n",
        "print(\"\\nDétails de l'instance à prédire:\")\n",
        "print(f\"Valeur réelle de consommation: {real_value:.2f} kWh\")\n",
        "\n",
        "print(\"\\nCaractéristiques importantes de l'instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # On n'affiche que les valeurs non nulles\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "# Prédiction de chaque arbre\n",
        "print(\"\\nPrédictions des arbres individuels:\")\n",
        "predictions = []\n",
        "for i, tree in enumerate(rf.estimators_):\n",
        "    pred = tree.predict([example])[0]\n",
        "    predictions.append(pred)\n",
        "    # print(f\"\\nArbre {i+1}:\")\n",
        "    # print(f\"Prédiction: {pred:.2f} kWh\")\n",
        "\n",
        "    # Affichage du chemin de décision pour cet arbre\n",
        "    path = tree.decision_path([example])\n",
        "    feature_path = []\n",
        "    for node_id in path.indices:\n",
        "        if node_id == tree.tree_.children_left[path.indices[0]]:  # Si c'est une feuille\n",
        "            continue\n",
        "        feature = X_train.columns[tree.tree_.feature[node_id]]\n",
        "        threshold = tree.tree_.threshold[node_id]\n",
        "        value = example[feature]\n",
        "        direction = \"gauche\" if value <= threshold else \"droite\"\n",
        "        feature_path.append(f\"   {feature} ≤ {threshold:.2f} ? {value:.2f} → {direction}\")\n",
        "\n",
        "    # print(\"Chemin de décision:\")\n",
        "    # for step in feature_path:\n",
        "    #     print(step)\n",
        "\n",
        "# Prédiction finale (moyenne des arbres)\n",
        "# /!\\ Complétez les '...' pour obtenir la moyenne des predictions (Numpy) /!\\\n",
        "final_prediction = ...\n",
        "print(f\"\\nPrédiction finale (moyenne des arbres): {final_prediction:.2f} kWh\")\n",
        "\n",
        "# Métriques de performance\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"\\nMétriques de performance:\")\n",
        "print(f\"R² score: {r2_score(y_test, y_pred):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f}\")\n",
        "\n",
        "# Importance des variables\n",
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 5 variables les plus importantes:\")\n",
        "print(importance.head().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSOxNSa5Vfv"
      },
      "source": [
        "❓ **Questions :**\n",
        "1. Pourquoi le Random Forest performe-t-il mieux que l'arbre simple ?\n",
        "\n",
        "2. Comparez l'importance des variables entre Random Forest et arbre simple.\n",
        "   Laquelle des deux estimations vous semble plus fiable ? Pourquoi ?\n",
        "\n",
        "3. **Mécanisme de vote**\n",
        "   - Pourquoi prendre la moyenne des prédictions des arbres ?\n",
        "\n",
        "4. **Avantages par rapport à l'arbre simple**\n",
        "   - Comment le Random Forest évite-t-il le surapprentissage ?\n",
        "   - Pourquoi les prédictions sont-elles plus stables ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfT_iben5bQj"
      },
      "source": [
        "### 6. Réseaux de neurones (MLP)\n",
        "Le Multi-Layer Perceptron est un réseau de neurones capable de capturer des relations complexes\n",
        "entre les variables. Il est particulièrement efficace pour les problèmes non-linéaires.\n",
        "\n",
        "Points clés :\n",
        "- Grande capacité d'apprentissage\n",
        "- Capable de capturer des relations complexes\n",
        "- Nécessite un réglage fin des hyperparamètres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIV4GuAQ5b_x"
      },
      "outputs": [],
      "source": [
        "# Création d'un MLP simple pour la visualisation\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(12, 6),\n",
        "                  max_iter=250,\n",
        "                  activation='relu',\n",
        "                  solver='adam',\n",
        "                  random_state=42)\n",
        "# /!\\ Complétez les '...' pour que le réseau de neurone apprenne à prédire y_train grâce aux X_train/!\\\n",
        "mlp...\n",
        "\n",
        "# Exemple de prédiction détaillée\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "prediction = mlp.predict([example])[0]\n",
        "\n",
        "print(\"\\nDétails de l'instance à prédire:\")\n",
        "print(f\"Valeur réelle de consommation: {real_value:.2f} kWh\")\n",
        "print(f\"Valeur prédite: {prediction:.2f} kWh\")\n",
        "\n",
        "print(\"\\nCaractéristiques importantes de l'instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # On n'affiche que les valeurs non nulles\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "# Test de différentes architectures\n",
        "architectures = [(12,), (25,), (12, 6), (25, 12), (25, 12, 6)]\n",
        "mlp_scores = []\n",
        "mlp_predictions = []\n",
        "\n",
        "print(\"\\nComparaison des architectures:\")\n",
        "for arch in architectures:\n",
        "    # Création et entraînement du modèle\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=arch,\n",
        "                      max_iter=250,\n",
        "                      random_state=42)\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    # Calcul du nombre de paramètres\n",
        "    n_params = sum(layer.size * next_layer.size + next_layer.size\n",
        "                  for layer, next_layer in zip([np.array([X_train.shape[1]])] + mlp.coefs_[:-1],\n",
        "                                             mlp.coefs_))\n",
        "\n",
        "    # Prédiction pour l'exemple\n",
        "    pred = mlp.predict([example])[0]\n",
        "    mlp_predictions.append(pred)\n",
        "\n",
        "    # Score global\n",
        "    score = r2_score(y_test, mlp.predict(X_test))\n",
        "    mlp_scores.append(score)\n",
        "\n",
        "    print(f\"\\nArchitecture {arch}:\")\n",
        "    print(f\"- Nombre de neurones par couche: Entrée({X_train.shape[1]}) → {' → '.join(str(x) for x in arch)} → Sortie(1)\")\n",
        "    print(f\"- Nombre total de paramètres: {n_params:,}\")\n",
        "    print(f\"- Prédiction pour l'exemple: {pred:.2f} kWh\")\n",
        "    print(f\"- R² score global: {score:.3f}\")\n",
        "\n",
        "# Visualisation des résultats\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Scores R²\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(architectures)), mlp_scores, 'bo-')\n",
        "plt.xticks(range(len(architectures)), [str(arch) for arch in architectures], rotation=45)\n",
        "plt.xlabel('Architecture')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance selon l\\'architecture')\n",
        "plt.grid(True)\n",
        "\n",
        "# Prédictions pour l'exemple\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(architectures)), mlp_predictions, 'ro-', label='Prédictions')\n",
        "plt.axhline(y=real_value, color='g', linestyle='--', label='Valeur réelle')\n",
        "plt.xticks(range(len(architectures)), [str(arch) for arch in architectures], rotation=45)\n",
        "plt.xlabel('Architecture')\n",
        "plt.ylabel('Prédiction (kWh)')\n",
        "plt.title('Prédictions pour l\\'exemple')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra7GoPP25ewn"
      },
      "source": [
        "❓ **Questions sur le MLP:**\n",
        "\n",
        "1. **Architecture du réseau**\n",
        "   - Pourquoi utiliser plusieurs couches cachées ?\n",
        "   - Comment choisir le nombre de neurones par couche ?\n",
        "\n",
        "2. **Comparaison des architectures**\n",
        "   - Quelle architecture donne les meilleurs résultats ? Pourquoi ?\n",
        "   - Y a-t-il un compromis entre complexité et performance ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhWHq34n5iA2"
      },
      "source": [
        "### 7. Comparaison finale des modèles\n",
        "\n",
        "Comparons maintenant tous les modèles pour choisir le plus adapté à notre problème."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGYPWnLf5ihH"
      },
      "outputs": [],
      "source": [
        "# Création et entraînement des meilleurs modèles\n",
        "# 1. Régression linéaire \n",
        "lr_sk = LinearRegression()\n",
        "lr_sk.fit(X_train, y_train)\n",
        "\n",
        "# 2. KNN \n",
        "knn_best = KNeighborsRegressor(n_neighbors=3)  # k=3 donnait les meilleurs résultats\n",
        "knn_best.fit(X_train, y_train)\n",
        "\n",
        "# 3. Arbre de décision \n",
        "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# 4. Random Forest \n",
        "rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# 5. MLP \n",
        "mlp = MLPRegressor(hidden_layer_sizes=(25, 12), max_iter=250, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "\n",
        "# Dictionnaire des modèles\n",
        "models = {\n",
        "    'Régression linéaire': lr_sk,\n",
        "    'KNN': knn_best,\n",
        "    'Arbre de décision': dt,\n",
        "    'Random Forest': rf,\n",
        "    'MLP': mlp\n",
        "}\n",
        "\n",
        "# Comparaison détaillée\n",
        "print(\"\\nPrédictions pour l'exemple (consommation réelle: {:.2f} kWh):\".format(real_value))\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    pred = model.predict([example])[0]\n",
        "    predictions[name] = pred\n",
        "    print(f\"{name}: {pred:.2f} kWh\")\n",
        "\n",
        "# Calcul des métriques globales\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    results.append({\n",
        "        'Modèle': name,\n",
        "        'R²': r2_score(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    })\n",
        "\n",
        "# /!\\ Complétez les '...' pour affecter à result_df un dataframe contenant les résultats results /!\\\n",
        "results_df = ...\n",
        "print(\"\\nMétriques globales:\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Visualisation des comparaisons\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# 1. Prédictions sur l'exemple\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(predictions.keys(), predictions.values(), color='skyblue')\n",
        "plt.axhline(y=real_value, color='r', linestyle='--', label='Valeur réelle')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Prédiction (kWh)')\n",
        "plt.title('Prédictions des modèles pour l\\'exemple')\n",
        "plt.legend()\n",
        "\n",
        "# 2. Scores R²\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(results_df['Modèle'], results_df['R²'], color='lightgreen')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance globale des modèles (R²)')\n",
        "# Ajuster l'échelle pour mieux voir les différences\n",
        "plt.ylim(0.98, 1.0)  # Les scores R² sont tous > 0.98\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "# 3. MSE\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.bar(results_df['Modèle'], results_df['MSE'], color='salmon')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Erreur quadratique moyenne (MSE)')\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AYceYGl8tZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po11pDRI5lMx"
      },
      "source": [
        "❓ **Analyse comparative des modèles:**\n",
        "\n",
        "1. **Compromis Complexité/Interprétabilité**\n",
        "   - Dans un contexte industriel, est-il préférable d'utiliser un modèle simple comme l'arbre de décision ou un modèle plus complexe comme le MLP ?\n",
        "   - Comment justifier le choix du modèle ?\n",
        "   - Quels sont les avantages et inconvénients d'utiliser un modèle \"boîte noire\" comme le MLP dans un environnement industriel ?\n",
        "\n",
        "2. **Aspects Pratiques et Opérationnels**\n",
        "   - Comment gérer la mise à jour des modèles avec l'arrivée de nouvelles données ?\n",
        "\n",
        "3. **Optimisation et Amélioration**\n",
        "   - Comment utiliser ces prédictions pour optimiser la consommation énergétique ?\n",
        "   - Quelles recommandations concrètes peut-on faire à cet industriel ?\n",
        "\n",
        "4. **Robustesse et Maintenance**\n",
        "   - Comment s'assurer que les modèles restent performants dans le temps ?\n",
        "   - À quelle fréquence réentraîner les modèles ?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
