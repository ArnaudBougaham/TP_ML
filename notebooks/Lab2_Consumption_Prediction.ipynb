{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwIAnRRJ3izu"
      },
      "source": [
        "# Machine Learning Lab - Steel Industry\n",
        "## Part 2: Energy Consumption Prediction\n",
        "\n",
        "In this part, we will develop machine learning models to predict the energy consumption of a steel industry. This predictive capability is crucial for cost optimization and production planning.\n",
        "\n",
        "### Objectives:\n",
        "- Implement different regression models\n",
        "- Compare their performance\n",
        "- Interpret the results in an industrial context\n",
        "\n",
        "### Data structure:\n",
        "\n",
        "1. **Target variable (to predict):**\n",
        "   - `Usage_kWh`: Energy consumption in kilowatt-hours\n",
        "\n",
        "2. **Numerical descriptive variables:**\n",
        "   - `Lagging_Current_Reactive.Power_kVarh`: Lagging reactive power\n",
        "   - `Leading_Current_Reactive_Power_kVarh`: Leading reactive power\n",
        "   - `CO2(tCO2)`: CO2 emissions\n",
        "   - `Lagging_Current_Power_Factor`: Lagging power factor\n",
        "   - `Leading_Current_Power_Factor`: Leading power factor\n",
        "   - `NSM`: Number of Seconds from Midnight (time)\n",
        "\n",
        "3. **Categorical variables:**\n",
        "   - `Day_of_week`: Day of the week (Monday to Sunday)\n",
        "   - `WeekStatus`: Type of day (Weekday/Weekend)\n",
        "\n",
        "### Applied preprocessing:\n",
        "1. Standardization of numerical variables (mean=0, std=1)\n",
        "2. One-hot encoding of categorical variables\n",
        "3. Train/test split (80%/20%)\n",
        "\n",
        "### Models covered:\n",
        "- Linear regression (baseline)\n",
        "- K-Nearest Neighbors (KNN)\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC74lzPz3nlb"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                           explained_variance_score, max_error)\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from scipy import stats\n",
        "import os\n",
        "\n",
        "# Display configuration\n",
        "sns.set_theme()  # Use seaborn style directly\n",
        "plt.rcParams['figure.figsize'] = [10, 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWz0Bfck3xSw"
      },
      "outputs": [],
      "source": [
        "# Download and prepare data\n",
        "if not os.path.exists('Steel_industry_data.csv'):\n",
        "    print(\"Downloading data...\")\n",
        "    # Download the zip file\n",
        "    !wget -O steel_industry_data.zip https://archive.ics.uci.edu/static/public/851/steel+industry+energy+consumption.zip\n",
        "    # Unzip the file\n",
        "    !unzip -o steel_industry_data.zip\n",
        "    print(\"Data downloaded and unzipped.\")\n",
        "else:\n",
        "    print(\"Data file already present.\")\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    df = pd.read_csv('Steel_industry_data.csv')\n",
        "    print(f\"Data loaded successfully: {df.shape[0]} observations, {df.shape[1]} variables\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# Separate variables\n",
        "target = 'Usage_kWh'\n",
        "numeric_features = [\n",
        "    'Lagging_Current_Reactive.Power_kVarh',\n",
        "    'Leading_Current_Reactive_Power_kVarh',\n",
        "    'CO2(tCO2)',\n",
        "    'Lagging_Current_Power_Factor',\n",
        "    'Leading_Current_Power_Factor',\n",
        "    'NSM'\n",
        "]\n",
        "categorical_features = ['Day_of_week', 'WeekStatus', 'period']\n",
        "\n",
        "# Display dataset dimensions\n",
        "print(\"Dataset dimensions:\")\n",
        "print(f\"Number of observations: {df.shape[0]:,}\")\n",
        "print(f\"Number of variables: {df.shape[1]:,}\")\n",
        "\n",
        "# Statistical summary of numerical variables\n",
        "print(\"\\nStatistical summary of numerical variables:\")\n",
        "display(df[numeric_features + [target]].describe())\n",
        "\n",
        "# Target variable distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x=target, bins=50)\n",
        "# /!\\ Complete the '...' to display as the figure title (Matplotlib): Energy consumption distribution /!\\\n",
        "plt...\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsktu0Ukxxz9"
      },
      "source": [
        "### 1. Temporal analysis and feature creation\n",
        "\n",
        "We will structure our temporal data at several levels:\n",
        "1. Days of the week (Monday to Sunday)\n",
        "2. Type of day (weekday/weekend)\n",
        "3. Periods of the day (6 blocks of 4 hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jo6Hh2Hx45f"
      },
      "outputs": [],
      "source": [
        "# Creation of industrial day periods\n",
        "def create_industrial_periods(df):\n",
        "    # Convert NSM to hours\n",
        "    df['hour'] = df['NSM'] / 3600\n",
        "\n",
        "    # Create periods with the day starting at 6am\n",
        "    conditions = [\n",
        "        (df['hour'] >= 6) & (df['hour'] < 10),   # Morning1\n",
        "        (df['hour'] >= 10) & (df['hour'] < 14),  # Morning2\n",
        "        (df['hour'] >= 14) & (df['hour'] < 18),  # Afternoon1\n",
        "        (df['hour'] >= 18) & (df['hour'] < 22),  # Afternoon2\n",
        "        (df['hour'] >= 22) | (df['hour'] < 2),   # Night1\n",
        "        (df['hour'] >= 2) & (df['hour'] < 6)     # Night2\n",
        "    ]\n",
        "\n",
        "    periods = ['Morning1', 'Morning2', 'Afternoon1', 'Afternoon2', 'Night1', 'Night2']\n",
        "    df['period'] = np.select(conditions, periods, default='Night2')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply periods\n",
        "# /!\\ Complete the '...' to transform df using the create_industrial_periods() function /!\\\n",
        "df = ...\n",
        "\n",
        "# Visualization of consumption patterns\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 1. Average consumption by period\n",
        "plt.subplot(1, 3, 1)\n",
        "period_order = ['Morning1', 'Morning2', 'Afternoon1', 'Afternoon2', 'Night1', 'Night2']\n",
        "sns.boxplot(data=df, x='period', y='Usage_kWh', order=period_order)\n",
        "plt.title('Consumption distribution by period')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 2. Heatmap period x day\n",
        "pivot_period_day = pd.pivot_table(df,\n",
        "                                values='Usage_kWh',\n",
        "                                index='period',\n",
        "                                columns='Day_of_week',\n",
        "                                aggfunc='mean')\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(pivot_period_day, cmap='YlOrRd', annot=True, fmt='.0f')\n",
        "plt.title('Average consumption\\nby period and day')\n",
        "\n",
        "# 3. Weekday/weekend comparison\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.boxplot(data=df, x='period', y='Usage_kWh', hue='WeekStatus', order=period_order)\n",
        "plt.title('Consumption by period\\nand day type')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed statistics\n",
        "print(\"\\nAverage consumption (kWh) by period and day type:\")\n",
        "pivot_stats = pd.pivot_table(df,\n",
        "                           values='Usage_kWh',\n",
        "                           index='period',\n",
        "                           columns=['WeekStatus', 'Day_of_week'],\n",
        "                           aggfunc=['mean', 'std'])\n",
        "display(pivot_stats.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsBi5wc9x8ye"
      },
      "source": [
        "❓ **Questions about temporal patterns:**\n",
        "\n",
        "1. **Production cycles**\n",
        "   - Which period shows the highest consumption? Why?\n",
        "   - How does consumption evolve between Morning1 and Morning2?\n",
        "\n",
        "2. **Day/night variations**\n",
        "   - What is the difference in consumption between daytime and nighttime periods?\n",
        "   - Is variability greater during the day or at night?\n",
        "\n",
        "3. **Weekend impact**\n",
        "   - How does the consumption pattern change on weekends?\n",
        "   - Which periods show the greatest weekday/weekend difference?\n",
        "   - What recommendations for energy optimization?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HK6q4HHxwku"
      },
      "outputs": [],
      "source": [
        "# Data preparation\n",
        "\n",
        "# 1. Standardization of numerical variables\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df[numeric_features]),\n",
        "    columns=numeric_features\n",
        ")\n",
        "\n",
        "# 2. Encoding of categorical variables\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "categorical_encoded = encoder.fit_transform(df[categorical_features])\n",
        "\n",
        "# Encoded column names\n",
        "day_names = [f'Day_{day}' for day in encoder.categories_[0]]\n",
        "week_status_names = [f'Status_{status}' for status in encoder.categories_[1]]\n",
        "period_names = [f'Period_{period}' for period in encoder.categories_[2]]\n",
        "encoded_columns = day_names + week_status_names + period_names\n",
        "\n",
        "# Check dimensions\n",
        "print(\"\\nEncoding dimensions:\")\n",
        "print(f\"Number of encoded columns: {len(encoded_columns)}\")\n",
        "print(f\"Shape of encoded data: {categorical_encoded.shape}\")\n",
        "print(\"Encoded categories:\")\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    print(f\"{feature}: {list(encoder.categories_[i])}\")\n",
        "\n",
        "# Create DataFrame with encoded variables\n",
        "df_encoded = pd.DataFrame(categorical_encoded, columns=encoded_columns)\n",
        "\n",
        "# 3. Combine features\n",
        "X = pd.concat([df_scaled, df_encoded], axis=1)\n",
        "# /!\\ Complete the '...' to assign the variable y to the target column of the dataframe /!\\\n",
        "y = ...\n",
        "\n",
        "print(\"Structure of prepared data:\")\n",
        "print(f\"Standardized numerical variables: {len(numeric_features)}\")\n",
        "print(f\"Encoded categorical variables: {len(encoded_columns)}\")\n",
        "print(f\"Final dimensions of X: {X.shape}\")\n",
        "\n",
        "# 4. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nTrain and test set dimensions:\")\n",
        "print(f\"X_train : {X_train.shape}\")\n",
        "print(f\"X_test : {X_test.shape}\")\n",
        "print(f\"y_train : {y_train.shape}\")\n",
        "print(f\"y_test : {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzGq21B45DO"
      },
      "source": [
        "### 2. Linear regression\n",
        "\n",
        "To understand how linear regression finds its coefficients, let's implement our own version:\n",
        "\n",
        "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
        "\n",
        "The loss (mean squared error) is:\n",
        "L = (1/n) Σ(y_pred - y_true)²\n",
        "\n",
        "The coefficients are updated as:\n",
        "β_new = β_old - α * ∂L/∂β\n",
        "where α is the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUrczzCX46-W"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        self.weights_history = []\n",
        "        self.bias_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Parameter initialization\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # History for visualization\n",
        "        self.loss_history = []\n",
        "        self.weights_history = []\n",
        "        self.bias_history = []\n",
        "\n",
        "        # Gradient descent\n",
        "        for i in range(self.n_iterations):\n",
        "            # Current prediction\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            # Compute and save loss and parameters\n",
        "            loss = np.mean((y_pred - y) ** 2)\n",
        "            self.loss_history.append(loss)\n",
        "            self.weights_history.append(self.weights.copy())\n",
        "            self.bias_history.append(self.bias)\n",
        "\n",
        "            # Display progress\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(f'Iteration {i+1}/{self.n_iterations}, Loss: {loss:.4f}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Visualization of error as a function of each weight\n",
        "def plot_error_vs_weights(X, y, weights, bias, feature_names, n_points=100):\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    n_features = len(weights)\n",
        "    n_rows = (n_features + 3) // 4  # Ceiling division for number of rows\n",
        "\n",
        "    for i, (feature_name, weight) in enumerate(zip(feature_names, weights)):\n",
        "        # Create a range of values around the optimal weight\n",
        "        weight_range = np.linspace(weight - 2, weight + 2, n_points)\n",
        "        errors = []\n",
        "\n",
        "        # Compute error for each weight value\n",
        "        for w in weight_range:\n",
        "            weights_temp = weights.copy()\n",
        "            weights_temp[i] = w\n",
        "            y_pred = np.dot(X, weights_temp) + bias\n",
        "            mse = np.mean((y - y_pred) ** 2)\n",
        "            errors.append(mse)\n",
        "\n",
        "        # Plot error curve\n",
        "        plt.subplot(n_rows, 4, i+1)\n",
        "        plt.plot(weight_range, errors, 'b-', alpha=0.7)\n",
        "        plt.axvline(x=weight, color='r', linestyle='--', label=f'w={weight:.2f}')\n",
        "        plt.title(f'MSE vs {feature_name}')\n",
        "        plt.xlabel('Weight value')\n",
        "        plt.ylabel('MSE')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Mark the found weight\n",
        "        plt.plot(weight, np.min(errors), 'ro', label='Found weight')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Train both models\n",
        "lr_gd = LinearRegressionGD(learning_rate=0.01, n_iterations=15000)\n",
        "# /!\\ Complete the '...' to apply linear regression lr_gd to the data /!\\\n",
        "...(X_train.values, y_train.values)\n",
        "\n",
        "lr_sk = LinearRegression()\n",
        "# /!\\ Complete the '...' to apply linear regression lr_sk to the data /!\\\n",
        "...(X_train, y_train)\n",
        "\n",
        "# Performance comparison\n",
        "print(\"\\nComparison of the two implementations:\")\n",
        "comparison = pd.DataFrame(columns=['GD', 'Sklearn'])\n",
        "\n",
        "# Predictions\n",
        "y_pred_gd = lr_gd.predict(X_test.values)\n",
        "y_pred_sk = lr_sk.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "comparison.loc['R² score'] = [\n",
        "    r2_score(y_test, y_pred_gd),\n",
        "    r2_score(y_test, y_pred_sk)\n",
        "]\n",
        "comparison.loc['MSE'] = [\n",
        "    mean_squared_error(y_test, y_pred_gd),\n",
        "    mean_squared_error(y_test, y_pred_sk)\n",
        "]\n",
        "comparison.loc['RMSE'] = [\n",
        "    np.sqrt(mean_squared_error(y_test, y_pred_gd)),\n",
        "    np.sqrt(mean_squared_error(y_test, y_pred_sk))\n",
        "]\n",
        "\n",
        "print(\"\\nPerformance metrics:\")\n",
        "display(comparison.round(4))\n",
        "\n",
        "# Coefficient comparison\n",
        "coef_comparison = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'GD': lr_gd.weights,\n",
        "    'Sklearn': lr_sk.coef_,\n",
        "    'Difference': np.abs(lr_gd.weights - lr_sk.coef_)\n",
        "})\n",
        "\n",
        "print(\"\\nCoefficient comparison:\")\n",
        "display(coef_comparison.round(4))\n",
        "\n",
        "# Display the full equation (with the 5 most important coefficients)\n",
        "print(\"\\nRegression equation:\")\n",
        "print(f\"Usage_kWh = {lr_gd.bias:.2f}\", end=\" \")\n",
        "top_coefs = coef_comparison.assign(abs_coef=lambda x: np.abs(x['GD'])).nlargest(21, 'abs_coef')\n",
        "for _, row in top_coefs.iterrows():\n",
        "    print(f\"+ ({row['GD']:.2f} × {row['Feature']})\", end=\" \")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualization of error curves\n",
        "plot_error_vs_weights(X_train.values, y_train.values,\n",
        "                     lr_gd.weights, lr_gd.bias,\n",
        "                     feature_names=X_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32orerj2vryv"
      },
      "source": [
        "### Interpretation of metrics:\n",
        "\n",
        "1. **R² (Coefficient of determination)**\n",
        "   - Ranges between 0 and 1 (or negative if the model is very poor)\n",
        "   - The closer to 1, the better the model\n",
        "   - Represents the proportion of variance explained by the model\n",
        "   - An R² of 0.8 means the model explains 80% of the data variability\n",
        "\n",
        "2. **MSE (Mean Squared Error)**\n",
        "   - Mean of squared errors\n",
        "   - Heavily penalizes large errors\n",
        "   - Hard to interpret because the unit is squared\n",
        "\n",
        "3. **RMSE (Root Mean Squared Error)**\n",
        "   - Square root of the MSE\n",
        "   - Same unit as the target variable (kWh)\n",
        "   - Easier to interpret: average error in kWh\n",
        "   - Example: RMSE = 10 means an average error of ±10 kWh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-Zk-4T49kJ"
      },
      "source": [
        "❓ **In-depth questions about linear regression:**\n",
        "\n",
        "1. **Convergence**\n",
        "   - How does the loss evolve over iterations?\n",
        "   - Why is the decrease fast at first and then slower?\n",
        "   - How do you know if the global minimum is reached?\n",
        "\n",
        "2. **Comparison with sklearn**\n",
        "   - Are the coefficients similar?\n",
        "   - Why are there differences?\n",
        "\n",
        "3. **Mathematical understanding**\n",
        "   - Why use MSE (Mean Squared Error) as the loss function?\n",
        "\n",
        "4. **Coefficient analysis**\n",
        "   - Why does CO2(tCO2) have the largest coefficient (26.47)?\n",
        "   - Do negative coefficients mean a negative influence?\n",
        "\n",
        "5. **Error curves**\n",
        "   - Why do the curves have a parabolic shape?\n",
        "   - What does the width of the parabola mean for each feature?\n",
        "   - Why do some features have a greater impact?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNF3Ufc44_9N"
      },
      "source": [
        "### 3. K-Nearest Neighbors (KNN)\n",
        "The K-Nearest Neighbors algorithm is a non-parametric method that predicts consumption based on the k most similar observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqn-ohkS5CpK"
      },
      "outputs": [],
      "source": [
        "class SimpleKNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Simply memorizes the training data\"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        print(f\"Memorizing {len(X)} training observations\")\n",
        "\n",
        "    def predict_single(self, x, verbose=False):\n",
        "        \"\"\"Predicts for a single observation with option to display details\"\"\"\n",
        "        # Compute distances to all training points\n",
        "        distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
        "\n",
        "        # Find the k nearest neighbors\n",
        "        nearest_indices = np.argsort(distances)[:self.k]\n",
        "        nearest_distances = distances[nearest_indices]\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\nPrediction details:\")\n",
        "            print(f\"Observation to predict: {x}\")\n",
        "            print(\"\\nNearest neighbors found:\")\n",
        "            for i, (idx, dist) in enumerate(zip(nearest_indices, nearest_distances)):\n",
        "                print(f\"Neighbor {i+1}:\")\n",
        "                print(f\"- Distance: {dist:.2f}\")\n",
        "                print(f\"- Value: {self.y_train[idx]:.2f}\")\n",
        "\n",
        "        # Compute prediction (simple mean)\n",
        "        prediction = np.mean(self.y_train[nearest_indices])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nFinal prediction: {prediction:.2f}\")\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts for multiple observations\"\"\"\n",
        "        return np.array([self.predict_single(x) for x in X])\n",
        "\n",
        "# Test with different values of k\n",
        "k_values = [1, 2, 3, 4, 5]\n",
        "knn_scores = []\n",
        "\n",
        "# Create a simple example for visualization\n",
        "example_idx = 42  # An arbitrary index for the example\n",
        "\n",
        "for k in k_values:\n",
        "    knn = SimpleKNN(k=k)\n",
        "    knn.fit(X_train.values, y_train.values)\n",
        "\n",
        "    # Detailed prediction for the example\n",
        "    print(f\"\\nTest with k={k}:\")\n",
        "    example_pred = knn.predict_single(X_test.values[example_idx], verbose=True)\n",
        "\n",
        "    # Compute metrics\n",
        "    y_pred = knn.predict(X_test.values)\n",
        "    # /!\\ Complete the '...' to compute the R² between the true target value and the prediction /!\\\n",
        "    r2 = ...\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    knn_scores.append(r2)\n",
        "\n",
        "    print(f\"\\nGlobal metrics:\")\n",
        "    print(f\"R² score: {r2:.3f}\")\n",
        "    print(f\"MSE: {mse:.3f}\")\n",
        "    print(f\"RMSE: {rmse:.3f}\")\n",
        "\n",
        "# Visualization of the impact of k\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_values, knn_scores, 'bo-')\n",
        "plt.xlabel('Number of neighbors (k)')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance according to number of neighbors')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1MFZ0r5FOY"
      },
      "source": [
        "❓ **Questions about the KNN algorithm:**\n",
        "\n",
        "1. **Algorithm understanding**\n",
        "   - How does KNN make predictions for a new observation?\n",
        "   - Why is it important for variables to be standardized?\n",
        "\n",
        "2. **Choice of k**\n",
        "   - What happens if k is too small (k=1)?\n",
        "   - What happens if k is too large (k close to n)?\n",
        "   - Why do we observe an optimal k in the performance curve?\n",
        "\n",
        "3. **Interpretability**\n",
        "   - How to explain a KNN prediction to a user?\n",
        "   - Can we identify the most important variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htfMqB5h5J9b"
      },
      "source": [
        "### 4. Decision Trees\n",
        "Decision trees allow the creation of easily interpretable prediction rules.\n",
        "They can capture non-linear relationships and are particularly useful in an industrial context.\n",
        "\n",
        "Key points:\n",
        "- Transparent and interpretable model\n",
        "- Able to capture non-linear relationships\n",
        "- Risk of overfitting to be controlled\n",
        "\n",
        "The decision tree recursively splits the data into homogeneous subgroups by choosing the best variables and split thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKbXBp1F5Kqb"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create and train the tree with depth 4\n",
        "dt = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Visualization of the tree with better readability\n",
        "plt.figure(figsize=(30, 15))  # Large size for readability\n",
        "plot_tree(dt,\n",
        "          feature_names=X_train.columns,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=8,  # Font size\n",
        "          precision=2)  # Number of decimals for values\n",
        "plt.title('Decision Tree (depth=8)', fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "# Example of detailed prediction\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "prediction = dt.predict([example])[0]\n",
        "\n",
        "print(\"\\nDetails of the instance to predict:\")\n",
        "print(f\"\\nActual consumption value: {real_value:.2f} kWh\")\n",
        "print(f\"Predicted value: {prediction:.2f} kWh\")\n",
        "\n",
        "print(\"\\nNon-zero features of the instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # Only display non-zero values\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "print(\"\\nDetailed decision path:\")\n",
        "\n",
        "print(\"\\nLevel 1:\")\n",
        "print(\"   Test: CO2(tCO2) ≤ 0.22\")\n",
        "print(\"   Measured value: -0.71\")\n",
        "print(\"   Number of observations: 28032\")\n",
        "print(\"   Group mean: 27.29 kWh\")\n",
        "print(\"   → Left branch (condition true)\")\n",
        "\n",
        "print(\"\\nLevel 2:\")\n",
        "print(\"   Test: CO2(tCO2) ≤ -0.40\")\n",
        "print(\"   Measured value: -0.71\")\n",
        "print(\"   Number of observations: 18049\")\n",
        "print(\"   Group mean: 5.41 kWh\")\n",
        "print(\"   → Left branch (condition true)\")\n",
        "\n",
        "print(\"\\nLevel 3:\")\n",
        "print(\"   Test: Lagging_Current_Reactive.Power_kVarh ≤ 0.60\")\n",
        "print(\"   Measured value: -0.60\")\n",
        "print(\"   Number of observations: 16817\")\n",
        "print(\"   Group mean: 3.91 kWh\")\n",
        "print(\"   → Left branch (condition true)\")\n",
        "\n",
        "print(\"\\nLevel 4:\")\n",
        "print(\"   Test: Lagging_Current_Reactive.Power_kVarh ≤ -0.1\")\n",
        "print(\"   Measured value: -0.60\")\n",
        "print(\"   Number of observations: 16793\")\n",
        "print(\"   Group mean: 3.76 kWh\")\n",
        "print(\"   → Left branch (condition true)\")\n",
        "\n",
        "print(\"\\n→ Final leaf:\")\n",
        "print(\"   - Number of observations: 16788\")\n",
        "print(\"   - Predicted value: 3.75 kWh\")\n",
        "\n",
        "# Performance metrics\n",
        "y_pred = dt.predict(X_test)\n",
        "print(\"\\nPerformance metrics:\")\n",
        "print(f\"R² score: {r2_score(y_test, y_pred):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f}\")\n",
        "\n",
        "# Create and train the tree with depth 10\n",
        "# /!\\ Complete the '...' to train a decision tree with depth 10 /!\\\n",
        "dt = ...\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6VT5BrJ5N3B"
      },
      "source": [
        "❓ **Questions:**\n",
        "1. What are the most important variables according to the decision tree?\n",
        "\n",
        "2. How does tree depth influence performance?\n",
        "\n",
        "3. [OPTIONAL] **Tree structure**\n",
        "   - Why is CO2(tCO2) often chosen as the first split?\n",
        "   - How does the number of observations decrease at each level?\n",
        "   - What is the meaning of the average values in the nodes?\n",
        "\n",
        "4. [OPTIONAL] **Prediction process**\n",
        "   - How does the tree arrive at its final prediction?\n",
        "   - Why are predictions more accurate with greater depth?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxRKnsE-5SOX"
      },
      "source": [
        "### 5. Random Forest\n",
        "Random Forest is an ensemble of decision trees that improves generalization and prediction stability compared to a single tree.\n",
        "\n",
        "Key points:\n",
        "- Better generalization than a single tree\n",
        "- Robust estimation of variable importance\n",
        "- Reduced overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lClrkkG15S6Z"
      },
      "outputs": [],
      "source": [
        "# Create a simple Random Forest with 500 trees for visualization\n",
        "rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Example of detailed prediction\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "\n",
        "print(\"\\nDetails of the instance to predict:\")\n",
        "print(f\"Actual consumption value: {real_value:.2f} kWh\")\n",
        "\n",
        "print(\"\\nImportant features of the instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # Only display non-zero values\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "# Prediction of each tree\n",
        "print(\"\\nPredictions of individual trees:\")\n",
        "predictions = []\n",
        "for i, tree in enumerate(rf.estimators_):\n",
        "    pred = tree.predict([example])[0]\n",
        "    predictions.append(pred)\n",
        "    # print(f\"\\nTree {i+1}:\")\n",
        "    # print(f\"Prediction: {pred:.2f} kWh\")\n",
        "\n",
        "    # Display the decision path for this tree\n",
        "    path = tree.decision_path([example])\n",
        "    feature_path = []\n",
        "    for node_id in path.indices:\n",
        "        if node_id == tree.tree_.children_left[path.indices[0]]:  # If it's a leaf\n",
        "            continue\n",
        "        feature = X_train.columns[tree.tree_.feature[node_id]]\n",
        "        threshold = tree.tree_.threshold[node_id]\n",
        "        value = example[feature]\n",
        "        direction = \"left\" if value <= threshold else \"right\"\n",
        "        feature_path.append(f\"   {feature} ≤ {threshold:.2f} ? {value:.2f} → {direction}\")\n",
        "\n",
        "    # print(\"Decision path:\")\n",
        "    # for step in feature_path:\n",
        "    #     print(step)\n",
        "\n",
        "# Final prediction (mean of the trees)\n",
        "# /!\\ Complete the '...' to get the mean of the predictions (Numpy) /!\\\n",
        "final_prediction ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSOxNSa5Vfv"
      },
      "source": [
        "❓ **Questions:**\n",
        "1. Why does Random Forest perform better than a single tree?\n",
        "\n",
        "2. [OPTIONAL] Compare variable importance between Random Forest and a single tree.\n",
        "   Which estimation seems more reliable to you? Why?\n",
        "\n",
        "3. [OPTIONAL] **Voting mechanism**\n",
        "   - Why take the average of the predictions from the trees?\n",
        "\n",
        "4. **Advantages over a single tree**\n",
        "   - How does Random Forest avoid overfitting?\n",
        "   - Why are predictions more stable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfT_iben5bQj"
      },
      "source": [
        "### 6. Neural Networks (MLP)\n",
        "The Multi-Layer Perceptron is a neural network capable of capturing complex relationships between variables. It is particularly effective for non-linear problems.\n",
        "\n",
        "Key points:\n",
        "- High learning capacity\n",
        "- Able to capture complex relationships\n",
        "- Requires fine-tuning of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIV4GuAQ5b_x"
      },
      "outputs": [],
      "source": [
        "# Create a simple MLP for visualization\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(12, 6),\n",
        "                  max_iter=250,\n",
        "                  activation='relu',\n",
        "                  solver='adam',\n",
        "                  random_state=42)\n",
        "# /!\\ Complete the '...' so that the neural network learns to predict y_train from X_train /!\\\n",
        "mlp...\n",
        "\n",
        "# Example of detailed prediction\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "prediction = mlp.predict([example])[0]\n",
        "\n",
        "print(\"\\nDetails of the instance to predict:\")\n",
        "print(f\"Actual consumption value: {real_value:.2f} kWh\")\n",
        "print(f\"Predicted value: {prediction:.2f} kWh\")\n",
        "\n",
        "print(\"\\nImportant features of the instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # Only display non-zero values\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "# Test different architectures\n",
        "architectures = [(12,), (25,), (12, 6), (25, 12), (25, 12, 6)]\n",
        "mlp_scores = []\n",
        "mlp_predictions = []\n",
        "\n",
        "print(\"\\nArchitecture comparison:\")\n",
        "for arch in architectures:\n",
        "    # Create and train the model\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=arch,\n",
        "                      max_iter=250,\n",
        "                      random_state=42)\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    # Compute the number of parameters\n",
        "    n_params = sum(layer.size * next_layer.size + next_layer.size\n",
        "                  for layer, next_layer in zip([np.array([X_train.shape[1]])] + mlp.coefs_[:-1],\n",
        "                                             mlp.coefs_))\n",
        "\n",
        "    # Prediction for the example\n",
        "    pred = mlp.predict([example])[0]\n",
        "    mlp_predictions.append(pred)\n",
        "\n",
        "    # Global score\n",
        "    score = r2_score(y_test, mlp.predict(X_test))\n",
        "    mlp_scores.append(score)\n",
        "\n",
        "    print(f\"\\nArchitecture {arch}:\")\n",
        "    print(f\"- Number of neurons per layer: Input({X_train.shape[1]}) → {' → '.join(str(x) for x in arch)} → Output(1)\")\n",
        "    print(f\"- Total number of parameters: {n_params:,}\")\n",
        "    print(f\"- Prediction for the example: {pred:.2f} kWh\")\n",
        "    print(f\"- Global R² score: {score:.3f}\")\n",
        "\n",
        "# Visualization of results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# R² scores\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(architectures)), mlp_scores, 'bo-')\n",
        "plt.xticks(range(len(architectures)), [str(arch) for arch in architectures], rotation=45)\n",
        "plt.xlabel('Architecture')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance by architecture')\n",
        "plt.grid(True)\n",
        "\n",
        "# Predictions for the example\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(architectures)), mlp_predictions, 'ro-', label='Predictions')\n",
        "plt.axhline(y=real_value, color='g', linestyle='--', label='Actual value')\n",
        "plt.xticks(range(len(architectures)), [str(arch) for arch in architectures], rotation=45)\n",
        "plt.xlabel('Architecture')\n",
        "plt.ylabel('Prediction (kWh)')\n",
        "plt.title('Predictions for the example')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra7GoPP25ewn"
      },
      "source": [
        "❓ **Questions about the MLP:**\n",
        "\n",
        "1. **Network architecture**\n",
        "   - Why use multiple hidden layers?\n",
        "   - How to choose the number of neurons per layer?\n",
        "\n",
        "2. **Architecture comparison**\n",
        "   - Which architecture gives the best results? Why?\n",
        "   - [OPTIONAL] Is there a trade-off between complexity and performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhWHq34n5iA2"
      },
      "source": [
        "### 7. Final model comparison\n",
        "\n",
        "Let's now compare all the models to choose the most suitable for our problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGYPWnLf5ihH"
      },
      "outputs": [],
      "source": [
        "# Create and train the best models\n",
        "# 1. Linear regression (already created)\n",
        "lr_sk = LinearRegression()\n",
        "lr_sk.fit(X_train, y_train)\n",
        "\n",
        "# 2. KNN (with the best k found)\n",
        "knn_best = KNeighborsRegressor(n_neighbors=3)  # k=3 gave the best results\n",
        "knn_best.fit(X_train, y_train)\n",
        "\n",
        "# 3. Decision tree (already created)\n",
        "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# 4. Random Forest (already created)\n",
        "rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# 5. MLP (already created)\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(25, 12), max_iter=250, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "\n",
        "# Model dictionary\n",
        "models = {\n",
        "    'Linear regression': lr_sk,\n",
        "    'KNN': knn_best,\n",
        "    'Decision tree': dt,\n",
        "    'Random Forest': rf,\n",
        "    'MLP': mlp\n",
        "}\n",
        "\n",
        "# Detailed comparison\n",
        "print(\"\\nPredictions for the example (actual consumption: {:.2f} kWh):\".format(real_value))\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    pred = model.predict([example])[0]\n",
        "    predictions[name] = pred\n",
        "    print(f\"{name}: {pred:.2f} kWh\")\n",
        "\n",
        "# Compute global metrics\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'R²': r2_score(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    })\n",
        "\n",
        "# /!\\ Complete the '...' to assign to results_df a dataframe containing the results /!\\\n",
        "results_df = ...\n",
        "print(\"\\nGlobal metrics:\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Visualization of comparisons\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# 1. Predictions on the example\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(predictions.keys(), predictions.values(), color='skyblue')\n",
        "plt.axhline(y=real_value, color='r', linestyle='--', label='Actual value')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Prediction (kWh)')\n",
        "plt.title('Model predictions for the example')\n",
        "plt.legend()\n",
        "\n",
        "# 2. R² scores\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(results_df['Model'], results_df['R²'], color='lightgreen')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Global model performance (R²)')\n",
        "# Adjust scale to better see differences\n",
        "plt.ylim(0.98, 1.0)  # All R² scores are > 0.98\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "# 3. MSE\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.bar(results_df['Model'], results_df['MSE'], color='salmon')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Mean Squared Error (MSE)')\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AYceYGl8tZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po11pDRI5lMx"
      },
      "source": [
        "❓ **Comparative analysis of models:**\n",
        "\n",
        "1. **Complexity/Interpretability trade-off**\n",
        "   - In an industrial context, is it better to use a simple model like a decision tree or a more complex model like an MLP?\n",
        "   - How to justify the choice of model?\n",
        "   - What are the advantages and disadvantages of using a \"black box\" model like the MLP in an industrial environment?\n",
        "\n",
        "2. [OPTIONAL] **Practical and operational aspects**\n",
        "   - How to handle model updates as new data arrives?\n",
        "\n",
        "3. **Optimization and improvement**\n",
        "   - How can these predictions be used to optimize energy consumption?\n",
        "   - What concrete recommendations can be made to this industrial company?\n",
        "\n",
        "4. **Robustness and maintenance**\n",
        "   - How to ensure that models remain performant over time?\n",
        "   - How often should models be retrained?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
