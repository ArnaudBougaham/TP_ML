{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwIAnRRJ3izu"
      },
      "source": [
        "# Machine Learning Lab - Steel Industry\n",
        "## Part 2: Energy Consumption Prediction\n",
        "\n",
        "In this part, we will develop machine learning models to predict the energy consumption of a steel industry. This predictive capability is crucial for cost optimization and production planning.\n",
        "\n",
        "### Objectives:\n",
        "- Implement different regression models\n",
        "- Compare their performance\n",
        "- Interpret the results in an industrial context\n",
        "\n",
        "### Data structure:\n",
        "\n",
        "1. **Target variable (to predict):**\n",
        "   - `Usage_kWh`: Energy consumption in kilowatt-hours\n",
        "\n",
        "2. **Numerical descriptive variables:**\n",
        "   - `Lagging_Current_Reactive.Power_kVarh`: Lagging reactive power\n",
        "   - `Leading_Current_Reactive_Power_kVarh`: Leading reactive power\n",
        "   - `CO2(tCO2)`: CO2 emissions\n",
        "   - `Lagging_Current_Power_Factor`: Lagging power factor\n",
        "   - `Leading_Current_Power_Factor`: Leading power factor\n",
        "   - `NSM`: Number of Seconds from Midnight (time)\n",
        "\n",
        "3. **Categorical variables:**\n",
        "   - `Day_of_week`: Day of the week (Monday to Sunday)\n",
        "   - `WeekStatus`: Type of day (Weekday/Weekend)\n",
        "\n",
        "### Applied preprocessing:\n",
        "1. Standardization of numerical variables (mean=0, std=1)\n",
        "2. One-hot encoding of categorical variables\n",
        "3. Train/test split (80%/20%)\n",
        "\n",
        "### Models covered:\n",
        "- Linear regression (baseline)\n",
        "- K-Nearest Neighbors (KNN)\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC74lzPz3nlb"
      },
      "outputs": [],
      "source": [
        "# Import des packages nécessaires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,\n",
        "                           explained_variance_score, max_error)\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from scipy import stats\n",
        "import os\n",
        "\n",
        "# Configuration de l'affichage\n",
        "sns.set_theme()  # Utilisation du style seaborn directement\n",
        "plt.rcParams['figure.figsize'] = [10, 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWz0Bfck3xSw"
      },
      "outputs": [],
      "source": [
        "# Téléchargement et préparation des données\n",
        "if not os.path.exists('Steel_industry_data.csv'):\n",
        "    print(\"Téléchargement des données...\")\n",
        "    # Télécharger le fichier zip\n",
        "    !wget -O steel_industry_data.zip https://archive.ics.uci.edu/static/public/851/steel+industry+energy+consumption.zip\n",
        "    # Décompresser le fichier\n",
        "    !unzip -o steel_industry_data.zip\n",
        "    print(\"Données téléchargées et décompressées.\")\n",
        "else:\n",
        "    print(\"Fichier de données déjà présent.\")\n",
        "\n",
        "# Chargement des données\n",
        "try:\n",
        "    df = pd.read_csv('Steel_industry_data.csv')\n",
        "    print(f\"Données chargées avec succès : {df.shape[0]} observations, {df.shape[1]} variables\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement des données : {e}\")\n",
        "    raise\n",
        "\n",
        "# Séparation des variables\n",
        "target = 'Usage_kWh'\n",
        "numeric_features = [\n",
        "    'Lagging_Current_Reactive.Power_kVarh',\n",
        "    'Leading_Current_Reactive_Power_kVarh',\n",
        "    'CO2(tCO2)',\n",
        "    'Lagging_Current_Power_Factor',\n",
        "    'Leading_Current_Power_Factor',\n",
        "    'NSM'\n",
        "]\n",
        "categorical_features = ['Day_of_week', 'WeekStatus', 'period']\n",
        "\n",
        "# Affichage des dimensions\n",
        "print(\"Dimensions du dataset :\")\n",
        "print(f\"Nombre d'observations : {df.shape[0]:,}\")\n",
        "print(f\"Nombre de variables : {df.shape[1]:,}\")\n",
        "\n",
        "# Résumé statistique des variables numériques\n",
        "print(\"\\nRésumé statistique des variables numériques :\")\n",
        "display(df[numeric_features + [target]].describe())\n",
        "\n",
        "# Distribution de la variable cible\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=df, x=target, bins=50)\n",
        "# /!\\ Complétez les '...' pour afficher comme titre de la figure (Matplotlib): Distribution de la consommation énergétique /!\\\n",
        "plt...\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsktu0Ukxxz9"
      },
      "source": [
        "### 1. Temporal analysis and feature creation\n",
        "\n",
        "We will structure our temporal data at several levels:\n",
        "1. Days of the week (Monday to Sunday)\n",
        "2. Type of day (weekday/weekend)\n",
        "3. Periods of the day (6 blocks of 4 hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jo6Hh2Hx45f"
      },
      "outputs": [],
      "source": [
        "# Création des périodes de la journée industrielle\n",
        "def create_industrial_periods(df):\n",
        "    # Conversion NSM en heures\n",
        "    df['hour'] = df['NSM'] / 3600\n",
        "\n",
        "    # Création des périodes avec la journée commençant à 6h\n",
        "    conditions = [\n",
        "        (df['hour'] >= 6) & (df['hour'] < 10),   # Matin1\n",
        "        (df['hour'] >= 10) & (df['hour'] < 14),  # Matin2\n",
        "        (df['hour'] >= 14) & (df['hour'] < 18),  # Aprem1\n",
        "        (df['hour'] >= 18) & (df['hour'] < 22),  # Aprem2\n",
        "        (df['hour'] >= 22) | (df['hour'] < 2),   # Nuit1\n",
        "        (df['hour'] >= 2) & (df['hour'] < 6)     # Nuit2\n",
        "    ]\n",
        "\n",
        "    periods = ['Matin1', 'Matin2', 'Aprem1', 'Aprem2', 'Nuit1', 'Nuit2']\n",
        "    df['period'] = np.select(conditions, periods, default='Nuit2')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Application des périodes\n",
        "# /!\\ Complétez les '...' pour transformer df grâce à la fonction create_industrial_periods() /!\\\n",
        "df = ...\n",
        "\n",
        "# Visualisation des patterns de consommation\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# 1. Consommation moyenne par période\n",
        "plt.subplot(1, 3, 1)\n",
        "period_order = ['Matin1', 'Matin2', 'Aprem1', 'Aprem2', 'Nuit1', 'Nuit2']\n",
        "sns.boxplot(data=df, x='period', y='Usage_kWh', order=period_order)\n",
        "plt.title('Distribution de la consommation par période')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 2. Heatmap période x jour\n",
        "pivot_period_day = pd.pivot_table(df,\n",
        "                                values='Usage_kWh',\n",
        "                                index='period',\n",
        "                                columns='Day_of_week',\n",
        "                                aggfunc='mean')\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(pivot_period_day, cmap='YlOrRd', annot=True, fmt='.0f')\n",
        "plt.title('Consommation moyenne\\npar période et jour')\n",
        "\n",
        "# 3. Comparaison semaine/weekend\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.boxplot(data=df, x='period', y='Usage_kWh', hue='WeekStatus', order=period_order)\n",
        "plt.title('Consommation par période\\net type de jour')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistiques détaillées\n",
        "print(\"\\nConsommation moyenne (kWh) par période et type de jour :\")\n",
        "pivot_stats = pd.pivot_table(df,\n",
        "                           values='Usage_kWh',\n",
        "                           index='period',\n",
        "                           columns=['WeekStatus', 'Day_of_week'],\n",
        "                           aggfunc=['mean', 'std'])\n",
        "display(pivot_stats.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsBi5wc9x8ye"
      },
      "source": [
        "❓ **Questions about temporal patterns:**\n",
        "\n",
        "1. **Production cycles**\n",
        "   - Which period shows the highest consumption? Why?\n",
        "   - How does consumption evolve between Morning1 and Morning2?\n",
        "\n",
        "2. **Day/night variations**\n",
        "   - What is the difference in consumption between daytime and nighttime periods?\n",
        "   - Is variability greater during the day or at night?\n",
        "\n",
        "3. **Weekend impact**\n",
        "   - How does the consumption pattern change on weekends?\n",
        "   - Which periods show the greatest weekday/weekend difference?\n",
        "   - What recommendations for energy optimization?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HK6q4HHxwku"
      },
      "outputs": [],
      "source": [
        "# Préparation des données\n",
        "\n",
        "# 1. Standardisation des variables numériques\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(df[numeric_features]),\n",
        "    columns=numeric_features\n",
        ")\n",
        "\n",
        "# 2. Encodage des variables catégorielles\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "categorical_encoded = encoder.fit_transform(df[categorical_features])\n",
        "\n",
        "# Noms des colonnes encodées\n",
        "day_names = [f'Day_{day}' for day in encoder.categories_[0]]\n",
        "week_status_names = [f'Status_{status}' for status in encoder.categories_[1]]\n",
        "period_names = [f'Period_{period}' for period in encoder.categories_[2]]\n",
        "encoded_columns = day_names + week_status_names + period_names\n",
        "\n",
        "# Vérification des dimensions\n",
        "print(\"\\nDimensions de l'encodage :\")\n",
        "print(f\"Nombre de colonnes encodées : {len(encoded_columns)}\")\n",
        "print(f\"Shape des données encodées : {categorical_encoded.shape}\")\n",
        "print(\"Catégories encodées :\")\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    print(f\"{feature}: {list(encoder.categories_[i])}\")\n",
        "\n",
        "# Création du DataFrame avec les variables encodées\n",
        "df_encoded = pd.DataFrame(categorical_encoded, columns=encoded_columns)\n",
        "\n",
        "# 3. Combinaison des features\n",
        "X = pd.concat([df_scaled, df_encoded], axis=1)\n",
        "# /!\\ Complétez les '...' pour affecter la variable y à la colonne target du dataframe /!\\\n",
        "y = ...\n",
        "\n",
        "print(\"Structure des données préparées :\")\n",
        "print(f\"Variables numériques standardisées : {len(numeric_features)}\")\n",
        "print(f\"Variables catégorielles encodées : {len(encoded_columns)}\")\n",
        "print(f\"Dimensions finales de X : {X.shape}\")\n",
        "\n",
        "# 4. Division train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nDimensions des ensembles d'entraînement et de test :\")\n",
        "print(f\"X_train : {X_train.shape}\")\n",
        "print(f\"X_test : {X_test.shape}\")\n",
        "print(f\"y_train : {y_train.shape}\")\n",
        "print(f\"y_test : {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vzGq21B45DO"
      },
      "source": [
        "### 2. Linear regression\n",
        "\n",
        "To understand how linear regression finds its coefficients, let's implement our own version:\n",
        "\n",
        "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
        "\n",
        "The loss (mean squared error) is:\n",
        "L = (1/n) Σ(y_pred - y_true)²\n",
        "\n",
        "The coefficients are updated as:\n",
        "β_new = β_old - α * ∂L/∂β\n",
        "where α is the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUrczzCX46-W"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "        self.weights_history = []\n",
        "        self.bias_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialisation des paramètres\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Historique pour visualisation\n",
        "        self.loss_history = []\n",
        "        self.weights_history = []\n",
        "        self.bias_history = []\n",
        "\n",
        "        # Descente de gradient\n",
        "        for i in range(self.n_iterations):\n",
        "            # Prédiction courante\n",
        "            y_pred = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Calcul des gradients\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Mise à jour des paramètres\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "            # Calcul et sauvegarde de la loss et des paramètres\n",
        "            loss = np.mean((y_pred - y) ** 2)\n",
        "            self.loss_history.append(loss)\n",
        "            self.weights_history.append(self.weights.copy())\n",
        "            self.bias_history.append(self.bias)\n",
        "\n",
        "            # Affichage progression\n",
        "            if (i+1) % 1000 == 0:\n",
        "                print(f'Iteration {i+1}/{self.n_iterations}, Loss: {loss:.4f}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Visualisation de l'erreur en fonction de chaque poids\n",
        "def plot_error_vs_weights(X, y, weights, bias, feature_names, n_points=100):\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    n_features = len(weights)\n",
        "    n_rows = (n_features + 3) // 4  # Arrondi supérieur pour le nombre de lignes\n",
        "\n",
        "    for i, (feature_name, weight) in enumerate(zip(feature_names, weights)):\n",
        "        # Créer une plage de valeurs autour du poids optimal\n",
        "        weight_range = np.linspace(weight - 2, weight + 2, n_points)\n",
        "        errors = []\n",
        "\n",
        "        # Calculer l'erreur pour chaque valeur du poids\n",
        "        for w in weight_range:\n",
        "            weights_temp = weights.copy()\n",
        "            weights_temp[i] = w\n",
        "            y_pred = np.dot(X, weights_temp) + bias\n",
        "            mse = np.mean((y - y_pred) ** 2)\n",
        "            errors.append(mse)\n",
        "\n",
        "        # Tracer la courbe d'erreur\n",
        "        plt.subplot(n_rows, 4, i+1)\n",
        "        plt.plot(weight_range, errors, 'b-', alpha=0.7)\n",
        "        plt.axvline(x=weight, color='r', linestyle='--', label=f'w={weight:.2f}')\n",
        "        plt.title(f'MSE vs {feature_name}')\n",
        "        plt.xlabel('Valeur du poids')\n",
        "        plt.ylabel('MSE')\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Marquer le poids trouvé\n",
        "        plt.plot(weight, np.min(errors), 'ro', label='Poids trouvé')\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Entraînement des deux modèles\n",
        "lr_gd = LinearRegressionGD(learning_rate=0.01, n_iterations=15000)\n",
        "# /!\\ Complétez les '...' pour appliquer la régression linéaire lr_gd aux données /!\\\n",
        "...(X_train.values, y_train.values)\n",
        "\n",
        "lr_sk = LinearRegression()\n",
        "# /!\\ Complétez les '...' pour appliquer la régression linéaire lr_sk aux données /!\\\n",
        "...(X_train, y_train)\n",
        "\n",
        "# Comparaison des performances\n",
        "print(\"\\nComparaison des deux implémentations :\")\n",
        "comparison = pd.DataFrame(columns=['GD', 'Sklearn'])\n",
        "\n",
        "# Prédictions\n",
        "y_pred_gd = lr_gd.predict(X_test.values)\n",
        "y_pred_sk = lr_sk.predict(X_test)\n",
        "\n",
        "# Métriques\n",
        "comparison.loc['R² score'] = [\n",
        "    r2_score(y_test, y_pred_gd),\n",
        "    r2_score(y_test, y_pred_sk)\n",
        "]\n",
        "comparison.loc['MSE'] = [\n",
        "    mean_squared_error(y_test, y_pred_gd),\n",
        "    mean_squared_error(y_test, y_pred_sk)\n",
        "]\n",
        "comparison.loc['RMSE'] = [\n",
        "    np.sqrt(mean_squared_error(y_test, y_pred_gd)),\n",
        "    np.sqrt(mean_squared_error(y_test, y_pred_sk))\n",
        "]\n",
        "\n",
        "print(\"\\nMétriques de performance :\")\n",
        "display(comparison.round(4))\n",
        "\n",
        "# Comparaison des coefficients\n",
        "coef_comparison = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'GD': lr_gd.weights,\n",
        "    'Sklearn': lr_sk.coef_,\n",
        "    'Différence': np.abs(lr_gd.weights - lr_sk.coef_)\n",
        "})\n",
        "\n",
        "print(\"\\nComparaison des coefficients :\")\n",
        "display(coef_comparison.round(4))\n",
        "\n",
        "# Affichage de l'équation complète (avec les 5 coefficients les plus importants)\n",
        "print(\"\\nÉquation de régression :\")\n",
        "print(f\"Usage_kWh = {lr_gd.bias:.2f}\", end=\" \")\n",
        "top_coefs = coef_comparison.assign(abs_coef=lambda x: np.abs(x['GD'])).nlargest(21, 'abs_coef')\n",
        "for _, row in top_coefs.iterrows():\n",
        "    print(f\"+ ({row['GD']:.2f} × {row['Feature']})\", end=\" \")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualisation des courbes d'erreur\n",
        "plot_error_vs_weights(X_train.values, y_train.values,\n",
        "                     lr_gd.weights, lr_gd.bias,\n",
        "                     feature_names=X_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32orerj2vryv"
      },
      "source": [
        "### Interpretation of metrics:\n",
        "\n",
        "1. **R² (Coefficient of determination)**\n",
        "   - Ranges between 0 and 1 (or negative if the model is very poor)\n",
        "   - The closer to 1, the better the model\n",
        "   - Represents the proportion of variance explained by the model\n",
        "   - An R² of 0.8 means the model explains 80% of the data variability\n",
        "\n",
        "2. **MSE (Mean Squared Error)**\n",
        "   - Mean of squared errors\n",
        "   - Heavily penalizes large errors\n",
        "   - Hard to interpret because the unit is squared\n",
        "\n",
        "3. **RMSE (Root Mean Squared Error)**\n",
        "   - Square root of the MSE\n",
        "   - Same unit as the target variable (kWh)\n",
        "   - Easier to interpret: average error in kWh\n",
        "   - Example: RMSE = 10 means an average error of ±10 kWh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-Zk-4T49kJ"
      },
      "source": [
        "❓ **In-depth questions about linear regression:**\n",
        "\n",
        "1. **Convergence**\n",
        "   - How does the loss evolve over iterations?\n",
        "   - Why is the decrease fast at first and then slower?\n",
        "   - How do you know if the global minimum is reached?\n",
        "\n",
        "2. **Comparison with sklearn**\n",
        "   - Are the coefficients similar?\n",
        "   - Why are there differences?\n",
        "\n",
        "3. **Mathematical understanding**\n",
        "   - Why use MSE (Mean Squared Error) as the loss function?\n",
        "\n",
        "4. **Coefficient analysis**\n",
        "   - Why does CO2(tCO2) have the largest coefficient (26.47)?\n",
        "   - Do negative coefficients mean a negative influence?\n",
        "\n",
        "5. **Error curves**\n",
        "   - Why do the curves have a parabolic shape?\n",
        "   - What does the width of the parabola mean for each feature?\n",
        "   - Why do some features have a greater impact?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNF3Ufc44_9N"
      },
      "source": [
        "### 3. K-Nearest Neighbors (KNN)\n",
        "The K-Nearest Neighbors algorithm is a non-parametric method that predicts consumption based on the k most similar observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqn-ohkS5CpK"
      },
      "outputs": [],
      "source": [
        "class SimpleKNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Mémorise simplement les données d'entraînement\"\"\"\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        print(f\"Mémorisation de {len(X)} observations d'entraînement\")\n",
        "\n",
        "    def predict_single(self, x, verbose=False):\n",
        "        \"\"\"Prédit pour une seule observation avec option d'affichage des détails\"\"\"\n",
        "        # Calcul des distances avec tous les points d'entraînement\n",
        "        distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
        "\n",
        "        # Trouve les k plus proches voisins\n",
        "        nearest_indices = np.argsort(distances)[:self.k]\n",
        "        nearest_distances = distances[nearest_indices]\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\nDétails de la prédiction:\")\n",
        "            print(f\"Observation à prédire: {x}\")\n",
        "            print(\"\\nPlus proches voisins trouvés:\")\n",
        "            for i, (idx, dist) in enumerate(zip(nearest_indices, nearest_distances)):\n",
        "                print(f\"Voisin {i+1}:\")\n",
        "                print(f\"- Distance: {dist:.2f}\")\n",
        "                print(f\"- Valeur: {self.y_train[idx]:.2f}\")\n",
        "\n",
        "        # Calcul de la prédiction (moyenne simple)\n",
        "        prediction = np.mean(self.y_train[nearest_indices])\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nPrédiction finale: {prediction:.2f}\")\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Prédit pour plusieurs observations\"\"\"\n",
        "        return np.array([self.predict_single(x) for x in X])\n",
        "\n",
        "# Test avec différentes valeurs de k\n",
        "k_values = [1, 2, 3, 4, 5]\n",
        "knn_scores = []\n",
        "\n",
        "# Création d'un exemple simple pour visualisation\n",
        "example_idx = 42  # Un indice arbitraire pour l'exemple\n",
        "\n",
        "for k in k_values:\n",
        "    knn = SimpleKNN(k=k)\n",
        "    knn.fit(X_train.values, y_train.values)\n",
        "\n",
        "    # Prédiction détaillée pour l'exemple\n",
        "    print(f\"\\nTest avec k={k}:\")\n",
        "    example_pred = knn.predict_single(X_test.values[example_idx], verbose=True)\n",
        "\n",
        "    # Calcul des métriques\n",
        "    y_pred = knn.predict(X_test.values)\n",
        "    # /!\\ Complétez les '...' pour calculer le R² entre la valeur cible réel et la prédiction /!\\\n",
        "    r2 = ...\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    knn_scores.append(r2)\n",
        "\n",
        "    print(f\"\\nMétriques globales:\")\n",
        "    print(f\"R² score: {r2:.3f}\")\n",
        "    print(f\"MSE: {mse:.3f}\")\n",
        "    print(f\"RMSE: {rmse:.3f}\")\n",
        "\n",
        "# Visualisation de l'impact de k\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_values, knn_scores, 'bo-')\n",
        "plt.xlabel('Nombre de voisins (k)')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance selon le nombre de voisins')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1MFZ0r5FOY"
      },
      "source": [
        "❓ **Questions about the KNN algorithm:**\n",
        "\n",
        "1. **Algorithm understanding**\n",
        "   - How does KNN make predictions for a new observation?\n",
        "   - Why is it important for variables to be standardized?\n",
        "\n",
        "2. **Choice of k**\n",
        "   - What happens if k is too small (k=1)?\n",
        "   - What happens if k is too large (k close to n)?\n",
        "   - Why do we observe an optimal k in the performance curve?\n",
        "\n",
        "3. **Interpretability**\n",
        "   - How to explain a KNN prediction to a user?\n",
        "   - Can we identify the most important variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htfMqB5h5J9b"
      },
      "source": [
        "### 4. Decision Trees\n",
        "Decision trees allow the creation of easily interpretable prediction rules.\n",
        "They can capture non-linear relationships and are particularly useful in an industrial context.\n",
        "\n",
        "Key points:\n",
        "- Transparent and interpretable model\n",
        "- Able to capture non-linear relationships\n",
        "- Risk of overfitting to be controlled\n",
        "\n",
        "The decision tree recursively splits the data into homogeneous subgroups by choosing the best variables and split thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKbXBp1F5Kqb"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Création et entraînement de l'arbre avec profondeur 4\n",
        "dt = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Visualisation de l'arbre avec une meilleure lisibilité\n",
        "plt.figure(figsize=(30, 15))  # Grande taille pour la lisibilité\n",
        "plot_tree(dt,\n",
        "          feature_names=X_train.columns,\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=8,  # Taille de la police\n",
        "          precision=2)  # Nombre de décimales pour les valeurs\n",
        "plt.title('Arbre de décision (profondeur=8)', fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "# Exemple de prédiction détaillée\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "prediction = dt.predict([example])[0]\n",
        "\n",
        "print(\"\\nDétails de l'instance à prédire:\")\n",
        "print(f\"\\nValeur réelle de consommation: {real_value:.2f} kWh\")\n",
        "print(f\"Valeur prédite: {prediction:.2f} kWh\")\n",
        "\n",
        "print(\"\\nCaractéristiques non nulles de l'instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # On n'affiche que les valeurs non nulles\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "print(\"\\nChemin de décision détaillé:\")\n",
        "\n",
        "print(\"\\nNiveau 1:\")\n",
        "print(\"   Test: CO2(tCO2) ≤ 0.22\")\n",
        "print(\"   Valeur mesurée: -0.71\")\n",
        "print(\"   Nombre d'observations: 28032\")\n",
        "print(\"   Moyenne du groupe: 27.29 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\nNiveau 2:\")\n",
        "print(\"   Test: CO2(tCO2) ≤ -0.40\")\n",
        "print(\"   Valeur mesurée: -0.71\")\n",
        "print(\"   Nombre d'observations: 18049\")\n",
        "print(\"   Moyenne du groupe: 5.41 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\nNiveau 3:\")\n",
        "print(\"   Test: Lagging_Current_Reactive.Power_kVarh ≤ 0.60\")\n",
        "print(\"   Valeur mesurée: -0.60\")\n",
        "print(\"   Nombre d'observations: 16817\")\n",
        "print(\"   Moyenne du groupe: 3.91 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\nNiveau 4:\")\n",
        "print(\"   Test: Lagging_Current_Reactive.Power_kVarh ≤ -0.1\")\n",
        "print(\"   Valeur mesurée: -0.60\")\n",
        "print(\"   Nombre d'observations: 16793\")\n",
        "print(\"   Moyenne du groupe: 3.76 kWh\")\n",
        "print(\"   → Branche gauche (condition vraie)\")\n",
        "\n",
        "print(\"\\n→ Feuille finale:\")\n",
        "print(\"   - Nombre d'observations: 16788\")\n",
        "print(\"   - Valeur prédite: 3.75 kWh\")\n",
        "\n",
        "# Métriques de performance\n",
        "y_pred = dt.predict(X_test)\n",
        "print(\"\\nMétriques de performance:\")\n",
        "print(f\"R² score: {r2_score(y_test, y_pred):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f}\")\n",
        "\n",
        "\n",
        "# Création et entraînement de l'arbre avec profondeur 10\n",
        "# /!\\ Complétez les '...' pour entrainer un arbre de décision d'une profondeur de 10 /!\\\n",
        "dt = ...\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Métriques de performance\n",
        "y_pred = dt.predict(X_test)\n",
        "print(\"\\nMétriques de performance pour profondeur 10:\")\n",
        "print(f\"R² score: {r2_score(y_test, y_pred):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6VT5BrJ5N3B"
      },
      "source": [
        "❓ **Questions:**\n",
        "1. What are the most important variables according to the decision tree?\n",
        "\n",
        "2. How does tree depth influence performance?\n",
        "\n",
        "3. [OPTIONAL] **Tree structure**\n",
        "   - Why is CO2(tCO2) often chosen as the first split?\n",
        "   - How does the number of observations decrease at each level?\n",
        "   - What is the meaning of the average values in the nodes?\n",
        "\n",
        "4. [OPTIONAL] **Prediction process**\n",
        "   - How does the tree arrive at its final prediction?\n",
        "   - Why are predictions more accurate with greater depth?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxRKnsE-5SOX"
      },
      "source": [
        "### 5. Random Forest\n",
        "Random Forest is an ensemble of decision trees that improves generalization and prediction stability compared to a single tree.\n",
        "\n",
        "Key points:\n",
        "- Better generalization than a single tree\n",
        "- Robust estimation of variable importance\n",
        "- Reduced overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lClrkkG15S6Z"
      },
      "outputs": [],
      "source": [
        "# Création d'un Random Forest simple avec 500 arbres pour la visualisation\n",
        "rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Exemple de prédiction détaillée\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "\n",
        "print(\"\\nDétails de l'instance à prédire:\")\n",
        "print(f\"Valeur réelle de consommation: {real_value:.2f} kWh\")\n",
        "\n",
        "print(\"\\nCaractéristiques importantes de l'instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # On n'affiche que les valeurs non nulles\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "# Prédiction de chaque arbre\n",
        "print(\"\\nPrédictions des arbres individuels:\")\n",
        "predictions = []\n",
        "for i, tree in enumerate(rf.estimators_):\n",
        "    pred = tree.predict([example])[0]\n",
        "    predictions.append(pred)\n",
        "    # print(f\"\\nArbre {i+1}:\")\n",
        "    # print(f\"Prédiction: {pred:.2f} kWh\")\n",
        "\n",
        "    # Affichage du chemin de décision pour cet arbre\n",
        "    path = tree.decision_path([example])\n",
        "    feature_path = []\n",
        "    for node_id in path.indices:\n",
        "        if node_id == tree.tree_.children_left[path.indices[0]]:  # Si c'est une feuille\n",
        "            continue\n",
        "        feature = X_train.columns[tree.tree_.feature[node_id]]\n",
        "        threshold = tree.tree_.threshold[node_id]\n",
        "        value = example[feature]\n",
        "        direction = \"gauche\" if value <= threshold else \"droite\"\n",
        "        feature_path.append(f\"   {feature} ≤ {threshold:.2f} ? {value:.2f} → {direction}\")\n",
        "\n",
        "    # print(\"Chemin de décision:\")\n",
        "    # for step in feature_path:\n",
        "    #     print(step)\n",
        "\n",
        "# Prédiction finale (moyenne des arbres)\n",
        "# /!\\ Complétez les '...' pour obtenir la moyenne des predictions (Numpy) /!\\\n",
        "final_prediction = ...\n",
        "print(f\"\\nPrédiction finale (moyenne des arbres): {final_prediction:.2f} kWh\")\n",
        "\n",
        "# Métriques de performance\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"\\nMétriques de performance:\")\n",
        "print(f\"R² score: {r2_score(y_test, y_pred):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.1f}\")\n",
        "\n",
        "# Importance des variables\n",
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 5 variables les plus importantes:\")\n",
        "print(importance.head().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgSOxNSa5Vfv"
      },
      "source": [
        "❓ **Questions:**\n",
        "1. Why does Random Forest perform better than a single tree?\n",
        "\n",
        "2. [OPTIONAL] Compare variable importance between Random Forest and a single tree.\n",
        "   Which estimation seems more reliable to you? Why?\n",
        "\n",
        "3. [OPTIONAL] **Voting mechanism**\n",
        "   - Why take the average of the predictions from the trees?\n",
        "\n",
        "4. **Advantages over a single tree**\n",
        "   - How does Random Forest avoid overfitting?\n",
        "   - Why are predictions more stable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfT_iben5bQj"
      },
      "source": [
        "### 6. Neural Networks (MLP)\n",
        "The Multi-Layer Perceptron is a neural network capable of capturing complex relationships between variables. It is particularly effective for non-linear problems.\n",
        "\n",
        "Key points:\n",
        "- High learning capacity\n",
        "- Able to capture complex relationships\n",
        "- Requires fine-tuning of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIV4GuAQ5b_x"
      },
      "outputs": [],
      "source": [
        "# Création d'un MLP simple pour la visualisation\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(12, 6),\n",
        "                  max_iter=250,\n",
        "                  activation='relu',\n",
        "                  solver='adam',\n",
        "                  random_state=42)\n",
        "# /!\\ Complétez les '...' pour que le réseau de neurone apprenne à prédire y_train grâce aux X_train/!\\\n",
        "mlp...\n",
        "\n",
        "# Exemple de prédiction détaillée\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "prediction = mlp.predict([example])[0]\n",
        "\n",
        "print(\"\\nDétails de l'instance à prédire:\")\n",
        "print(f\"Valeur réelle de consommation: {real_value:.2f} kWh\")\n",
        "print(f\"Valeur prédite: {prediction:.2f} kWh\")\n",
        "\n",
        "print(\"\\nCaractéristiques importantes de l'instance:\")\n",
        "for feature, value in example.items():\n",
        "    if abs(value) > 0.01:  # On n'affiche que les valeurs non nulles\n",
        "        print(f\"{feature}: {value:.2f}\")\n",
        "\n",
        "# Test de différentes architectures\n",
        "architectures = [(12,), (25,), (12, 6), (25, 12), (25, 12, 6)]\n",
        "mlp_scores = []\n",
        "mlp_predictions = []\n",
        "\n",
        "print(\"\\nComparaison des architectures:\")\n",
        "for arch in architectures:\n",
        "    # Création et entraînement du modèle\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=arch,\n",
        "                      max_iter=250,\n",
        "                      random_state=42)\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    # Calcul du nombre de paramètres\n",
        "    n_params = sum(layer.size * next_layer.size + next_layer.size\n",
        "                  for layer, next_layer in zip([np.array([X_train.shape[1]])] + mlp.coefs_[:-1],\n",
        "                                             mlp.coefs_))\n",
        "\n",
        "    # Prédiction pour l'exemple\n",
        "    pred = mlp.predict([example])[0]\n",
        "    mlp_predictions.append(pred)\n",
        "\n",
        "    # Score global\n",
        "    score = r2_score(y_test, mlp.predict(X_test))\n",
        "    mlp_scores.append(score)\n",
        "\n",
        "    print(f\"\\nArchitecture {arch}:\")\n",
        "    print(f\"- Nombre de neurones par couche: Entrée({X_train.shape[1]}) → {' → '.join(str(x) for x in arch)} → Sortie(1)\")\n",
        "    print(f\"- Nombre total de paramètres: {n_params:,}\")\n",
        "    print(f\"- Prédiction pour l'exemple: {pred:.2f} kWh\")\n",
        "    print(f\"- R² score global: {score:.3f}\")\n",
        "\n",
        "# Visualisation des résultats\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Scores R²\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(architectures)), mlp_scores, 'bo-')\n",
        "plt.xticks(range(len(architectures)), [str(arch) for arch in architectures], rotation=45)\n",
        "plt.xlabel('Architecture')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance selon l\\'architecture')\n",
        "plt.grid(True)\n",
        "\n",
        "# Prédictions pour l'exemple\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(architectures)), mlp_predictions, 'ro-', label='Prédictions')\n",
        "plt.axhline(y=real_value, color='g', linestyle='--', label='Valeur réelle')\n",
        "plt.xticks(range(len(architectures)), [str(arch) for arch in architectures], rotation=45)\n",
        "plt.xlabel('Architecture')\n",
        "plt.ylabel('Prédiction (kWh)')\n",
        "plt.title('Prédictions pour l\\'exemple')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra7GoPP25ewn"
      },
      "source": [
        "❓ **Questions about the MLP:**\n",
        "\n",
        "1. **Network architecture**\n",
        "   - Why use multiple hidden layers?\n",
        "   - How to choose the number of neurons per layer?\n",
        "\n",
        "2. **Architecture comparison**\n",
        "   - Which architecture gives the best results? Why?\n",
        "   - [OPTIONAL] Is there a trade-off between complexity and performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhWHq34n5iA2"
      },
      "source": [
        "### 7. Final model comparison\n",
        "\n",
        "Let's now compare all the models to choose the most suitable for our problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGYPWnLf5ihH"
      },
      "outputs": [],
      "source": [
        "# Création et entraînement des meilleurs modèles\n",
        "# 1. Régression linéaire (déjà créée)\n",
        "lr_sk = LinearRegression()\n",
        "lr_sk.fit(X_train, y_train)\n",
        "\n",
        "# 2. KNN (avec le meilleur k trouvé)\n",
        "knn_best = KNeighborsRegressor(n_neighbors=3)  # k=3 donnait les meilleurs résultats\n",
        "knn_best.fit(X_train, y_train)\n",
        "\n",
        "# 3. Arbre de décision (déjà créé)\n",
        "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# 4. Random Forest (déjà créé)\n",
        "rf = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# 5. MLP (déjà créé)\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(25, 12), max_iter=250, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "example_idx = 42\n",
        "example = X_test.iloc[example_idx]\n",
        "real_value = y_test.iloc[example_idx]\n",
        "\n",
        "# Dictionnaire des modèles\n",
        "models = {\n",
        "    'Régression linéaire': lr_sk,\n",
        "    'KNN': knn_best,\n",
        "    'Arbre de décision': dt,\n",
        "    'Random Forest': rf,\n",
        "    'MLP': mlp\n",
        "}\n",
        "\n",
        "# Comparaison détaillée\n",
        "print(\"\\nPrédictions pour l'exemple (consommation réelle: {:.2f} kWh):\".format(real_value))\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    pred = model.predict([example])[0]\n",
        "    predictions[name] = pred\n",
        "    print(f\"{name}: {pred:.2f} kWh\")\n",
        "\n",
        "# Calcul des métriques globales\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    results.append({\n",
        "        'Modèle': name,\n",
        "        'R²': r2_score(y_test, y_pred),\n",
        "        'MSE': mean_squared_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    })\n",
        "\n",
        "# /!\\ Complétez les '...' pour affecter à result_df un dataframe contenant les résultats results /!\\\n",
        "results_df = ...\n",
        "print(\"\\nMétriques globales:\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Visualisation des comparaisons\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# 1. Prédictions sur l'exemple\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(predictions.keys(), predictions.values(), color='skyblue')\n",
        "plt.axhline(y=real_value, color='r', linestyle='--', label='Valeur réelle')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Prédiction (kWh)')\n",
        "plt.title('Prédictions des modèles pour l\\'exemple')\n",
        "plt.legend()\n",
        "\n",
        "# 2. Scores R²\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(results_df['Modèle'], results_df['R²'], color='lightgreen')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Performance globale des modèles (R²)')\n",
        "# Ajuster l'échelle pour mieux voir les différences\n",
        "plt.ylim(0.98, 1.0)  # Les scores R² sont tous > 0.98\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "# 3. MSE\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.bar(results_df['Modèle'], results_df['MSE'], color='salmon')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Erreur quadratique moyenne (MSE)')\n",
        "plt.grid(True, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AYceYGl8tZe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po11pDRI5lMx"
      },
      "source": [
        "❓ **Comparative analysis of models:**\n",
        "\n",
        "1. **Complexity/Interpretability trade-off**\n",
        "   - In an industrial context, is it better to use a simple model like a decision tree or a more complex model like an MLP?\n",
        "   - How to justify the choice of model?\n",
        "   - What are the advantages and disadvantages of using a \"black box\" model like the MLP in an industrial environment?\n",
        "\n",
        "2. [OPTIONAL] **Practical and operational aspects**\n",
        "   - How to handle model updates as new data arrives?\n",
        "\n",
        "3. **Optimization and improvement**\n",
        "   - How can these predictions be used to optimize energy consumption?\n",
        "   - What concrete recommendations can be made to this industrial company?\n",
        "\n",
        "4. **Robustness and maintenance**\n",
        "   - How to ensure that models remain performant over time?\n",
        "   - How often should models be retrained?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
